{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import shapely\n",
    "import folium\n",
    "import geojson\n",
    "import math\n",
    "import osmnx as ox\n",
    "from rtree import index as rtree_index\n",
    "import pickle\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString, Point\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "from __future__ import absolute_import, division\n",
    "from math import radians, sin, cos, sqrt, atan2, exp\n",
    "import webbrowser\n",
    "import random\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "ox.settings.log_console=True\n",
    "ox.settings.use_cache=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining classes for the dataframes\n",
    "class AmenityPoint:\n",
    "    def __init__(self, geometry, lat, lon, amenity, name, addr_city):\n",
    "        self.geometry = geometry\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.amenity = amenity\n",
    "        self.name = name\n",
    "        self.addr_city = addr_city\n",
    "\n",
    "class AmenityPolygon:\n",
    "    def __init__(self, geometry, lat, lon, amenity, name, addr_city):\n",
    "        self.geometry = geometry\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.amenity = amenity\n",
    "        self.name = name\n",
    "        self.addr_city = addr_city\n",
    "        \n",
    "class stopCandidate:\n",
    "    def __init__(self, lat, long, isTranspo):\n",
    "        self.lat = lat\n",
    "        self.long = long\n",
    "        self.isTranspo = isTranspo\n",
    "        self.enabled = False\n",
    "        \n",
    "    def enable(self):\n",
    "        self.enabled = True\n",
    "        \n",
    "    def disable(self):\n",
    "        self.enabled = False\n",
    "        \n",
    "    def getLat(self):\n",
    "        return self.lat\n",
    "    \n",
    "    def getLong(self):\n",
    "        return self.long\n",
    "    \n",
    "class network():\n",
    "    def __init__(self, routes, stops):\n",
    "        self.routes = routes\n",
    "        self.stops = stops\n",
    "        self.fitness_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For units\n",
    "def degrees_to_meters(angle_degrees):\n",
    "    return angle_degrees * 6371000 * math.pi / 180\n",
    "\n",
    "def meters_to_degrees(distance_meters):\n",
    "    return distance_meters / 6371000 * 180 / math.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the building footprints data\n",
    "\n",
    "#buildingfootprints_gdf = gpd.read_file('manila_building_footprints.geojson')\n",
    "\n",
    "#buildingfootprints_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING DATA (All Amenities in Manila)\n",
    "merged_amenities_points_gdf = gpd.read_file('./merged and cleaned amenities/merged_amenity_points/merged_amenities_points.shp', crs='epsg:3123')\n",
    "merged_amenities_polygons_gdf = gpd.read_file('./merged and cleaned amenities/merged_amenity_polygons/cleaned_merged_with_OSM.shp', crs='epsg:3123')\n",
    "\n",
    "merged_amenities_polygons_gdf['amenity_points'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD DATA (FOR FAST TESTING)\n",
    "# Load the Manila amenities data into a Geopandas dataframe\n",
    "from shapely import wkt\n",
    "\n",
    "manila_amenities_df = pd.read_csv('manila_amenities.csv')\n",
    "manila_amenities_df['geometry'] = manila_amenities_df['geometry'].apply(wkt.loads)\n",
    "manila_amenities_gdf = gpd.GeoDataFrame(manila_amenities_df, crs='epsg:3123')\n",
    "\n",
    "# Separate into point and polygon dataframes\n",
    "manila_amenities_polygon_gdf = manila_amenities_gdf[manila_amenities_gdf['geometry'].geom_type == 'Polygon']\n",
    "manila_amenities_point_gdf = manila_amenities_gdf[manila_amenities_gdf['geometry'].geom_type == 'Point']\n",
    "manila_amenities_multipoly_gdf = manila_amenities_gdf[manila_amenities_gdf['geometry'].geom_type == 'MultiPolygon']\n",
    "\n",
    "# Append multipolygons to the polygon dataframe\n",
    "manila_amenities_polygon_gdf = gpd.GeoDataFrame(pd.concat([manila_amenities_polygon_gdf, manila_amenities_multipoly_gdf], ignore_index=True))\n",
    "\n",
    "# Reset point dataframe index\n",
    "manila_amenities_point_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add a column to the polygon dataframe to store a list of Amenity Points within the polygon\n",
    "manila_amenities_polygon_gdf['amenity_points'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each polygon in the polygon dataframe, find all the points from the point dataframe lying inside that polygon\n",
    "# Store the list of points in the 'amenity_points' column of the polygon dataframe as a list of point indices\n",
    "\n",
    "# Create spatial index for points\n",
    "idx = rtree_index.Index()\n",
    "for j, point in manila_amenities_point_gdf.iterrows():\n",
    "    idx.insert(j, point['geometry'].bounds)\n",
    "\n",
    "# Iterate over polygons\n",
    "for i, polygon in manila_amenities_polygon_gdf.iterrows():\n",
    "    points_within_polygon = []\n",
    "    \n",
    "    # Iterate over points within the bounding box of the polygon\n",
    "    for j in idx.intersection(polygon['geometry'].bounds):\n",
    "        point = manila_amenities_point_gdf.loc[j]\n",
    "        if polygon['geometry'].intersects(point['geometry']):\n",
    "            points_within_polygon.append(j)\n",
    "    \n",
    "    manila_amenities_polygon_gdf.at[i, 'amenity_points'] = points_within_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['education', 'finance', 'government offices', 'grocery', 'health',\n",
       "       'malls', 'residential areas', 'security'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manila_amenities_polygon_gdf['amenity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['education', 'finance', 'government offices', nan, 'public_market',\n",
       "       'health', 'malls', 'residential areas', 'security',\n",
       "       'transportation'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manila_amenities_point_gdf['amenity'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading population data\n",
    "manila_population_df = pd.read_csv('manila-population-polygon.csv')\n",
    "manila_population_df['geometry'] = manila_population_df['geometry'].apply(wkt.loads)\n",
    "manila_population_gdf = gpd.GeoDataFrame(manila_population_df, crs='epsg:3123')\n",
    "\n",
    "# Create a base map centered around Manila\n",
    "map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "# Add points to the map\n",
    "for index, row in manila_population_gdf.iterrows():\n",
    "    folium.CircleMarker(location=[row['latitude'], row['longitude']],\n",
    "                        radius=1,  # Adjust the radius as needed for population density representation\n",
    "                        color='blue',  # Change color as needed\n",
    "                        fill=True,\n",
    "                        fill_color='blue'  # Change fill color as needed\n",
    "                        ).add_to(m)\n",
    "    \n",
    "m.save('population.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded successfully\n",
      "NUMBER OF EDGES:  12617\n",
      "NUMBER OF NODES:  4926\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GENERATION OF MAIN CITY GRAPH\n",
    "# IF FIRST TIME RUNNING, RUN THIS CODE TO GENERATE THE GRAPH\n",
    "def generate_graph():\n",
    "    place = 'Manila, Philippines'\n",
    "    mode = 'drive'\n",
    "    graph = ox.graph_from_place(place, network_type = mode) # Generate graph of Metro manila\n",
    "    ox.save_graphml(graph, 'map/Manila.graphml') # Save it as a file\n",
    "\n",
    "def load_graph():\n",
    "    graph = ox.load_graphml('map/Manila.graphml')\n",
    "    \n",
    "    print(\"Graph loaded successfully\")\n",
    "    print(\"NUMBER OF EDGES: \", graph.number_of_edges())\n",
    "    print(\"NUMBER OF NODES: \", graph.number_of_nodes())\n",
    "    print('\\n')\n",
    "    return graph\n",
    "\n",
    "\n",
    "# THIS IS THE MAIN GRAPH FOR THE CITY TO BE USED FOR ALL FUNCTIONS\n",
    "CITY_GRAPH = load_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buckle up. We're trying to create a network out of this monstrosity of a dataframe\n",
    "# Create a networkx graph\n",
    "\n",
    "def create_network(amenities_polygon_gdf, amenities_point_gdf):\n",
    "    amenities_network = nx.Graph()\n",
    "\n",
    "   # Add polygon nodes\n",
    "    for index, row in amenities_polygon_gdf.iterrows():\n",
    "        # Check if essential columns exist in the row\n",
    "        if 'geometry' in row and 'amenity' in row and 'name' in row and 'addr_city' in row and 'amenity_points' in row:\n",
    "            # Generate a unique node identifier for polygons\n",
    "            node_id = f\"polygon_{index}\"\n",
    "            amenities_network.add_node(node_id, polygon_index=index, geometry=row['geometry'], lat=row['geometry'].centroid.y, lon=row['geometry'].centroid.x, amenity=row['amenity'], name=row.get('name', ''), addr_city=row['addr_city'], amenity_points=row['amenity_points'])\n",
    "        else:\n",
    "            print(f\"Skipping row {index} in amenities_polygon_gdf due to missing data.\")\n",
    "\n",
    "    # Add point nodes\n",
    "    for index, row in amenities_point_gdf.iterrows():\n",
    "        # Check if essential columns exist in the row\n",
    "        if 'geometry' in row and 'amenity' in row and 'name' in row and 'addr_city' in row:\n",
    "            # Generate a unique node identifier for points\n",
    "            node_id = f\"point_{index}\"\n",
    "            \n",
    "            # This part checks whether the point is a transportation or not \n",
    "            if row['amenity'] == 'transportation':\n",
    "                isTranspo = True\n",
    "            else:\n",
    "                isTranspo = False\n",
    "            amenities_network.add_node(node_id, point_index=index, geometry=row['geometry'], lat=row['y'], lon=row['x'], amenity=row['amenity'], name=row.get('name', ''), addr_city=row['addr_city'], is_in_polygon=False, isTranspo = isTranspo)\n",
    "        else:\n",
    "            print(f\"Skipping row {index} in amenities_point_gdf due to missing data.\")\n",
    "            \n",
    "    return amenities_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Filtering the roads and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GETTING ROADS AND WATERWAYS\n",
    "\n",
    "# Get all the roads in Manila\n",
    "manila_road = ox.graph_to_gdfs(CITY_GRAPH,nodes=False, edges=True)\n",
    "\n",
    "\n",
    "# Get all the roads that are not junctions (ex. Roundabouts, intersection, etc.)\n",
    "filtered_roads = manila_road[manila_road['junction'].isna()]\n",
    "\n",
    "# Separate roads whose widths are only one value and those that are more than 1 (lists)\n",
    "rows_with_lists = filtered_roads[filtered_roads['highway'].apply(lambda x: isinstance(x, list))]\n",
    "rows_with_strings = filtered_roads[filtered_roads['highway'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "filter_options = ['primary', 'secondary', 'tertiary', 'trunk', 'unclassified']\n",
    "separation_options = ['primary', 'secondary', 'tertiary', 'unclassified']\n",
    "\n",
    "# Get the roads whose widths are above the threshold\n",
    "def check_list(lst):\n",
    "    return any(x in filter_options for x in lst)\n",
    "\n",
    "# Download OpenStreetMap data for the area of interest\n",
    "waterways = ox.features_from_place('Manila, Philippines', tags={'waterway': True})\n",
    "filtered_rivers = waterways[waterways['waterway'].isin(['river'])]\n",
    "filtered_streams = waterways[waterways['waterway'].isin(['stream'])]\n",
    "\n",
    "# Get all the roads with the allowed road types\n",
    "filtered_roads_strings = rows_with_strings.loc[rows_with_strings['highway'].isin(filter_options)] \n",
    "filtered_roads_lists = rows_with_lists[rows_with_lists['highway'].apply(check_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Roads and Rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting filtered roads FOR VISUALIZATION ONLY\n",
    "def plot_all_filtered_roads():\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate through each road\n",
    "    for index, road in filtered_roads_strings.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "\n",
    "        if road['highway'] == 'primary':\n",
    "            folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='blue').add_to(m)\n",
    "\n",
    "        if road['highway'] == 'secondary':\n",
    "            folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='red').add_to(m)\n",
    "\n",
    "        if road['highway'] == 'tertiary':\n",
    "            folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='green').add_to(m)\n",
    "\n",
    "        if road['highway'] == 'unclassified':\n",
    "            folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='orange').add_to(m)\n",
    "\n",
    "    for index, road in filtered_roads_lists.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "        folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='purple').add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n",
    "\n",
    "def plot_all_roads():\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate through each road\n",
    "    for index, road in manila_road.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "        folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='blue').add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n",
    "\n",
    "# TEMPORARY TO BE REMOVED\n",
    "def plot_private_roads():\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate through each road\n",
    "    for index, road in manila_private.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "        folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='blue').add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n",
    "\n",
    "# TEMPORARY TO BE REMOVED\n",
    "def plot_walk():\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate through each road\n",
    "    for index, road in manila_walk.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "        folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='blue').add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n",
    "    \n",
    "# TEMPORARY TO BE REMOVED\n",
    "def plot_bike():\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate through each road\n",
    "    for index, road in manila_bike.iterrows():\n",
    "        line_coords = list(road['geometry'].coords)\n",
    "        folium.PolyLine(locations=[(y, x) for x, y in line_coords], color='blue').add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Amenity and zone connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will find which road or river intersects between amenities\n",
    "# Create spatial index\n",
    "filtered_roads_strings_sindex = filtered_roads_strings.sindex\n",
    "filtered_roads_lists_sindex = filtered_roads_lists.sindex\n",
    "filtered_rivers_sindex = filtered_rivers.sindex\n",
    "filtered_streams_sindex = filtered_streams.sindex\n",
    "\n",
    "def find_intersecting_features(line):\n",
    "    # Check intersection with filtered roads\n",
    "    possible_matches_roads = filtered_roads_strings.iloc[list(filtered_roads_strings_sindex.intersection(line.bounds))]\n",
    "    for index, row in possible_matches_roads.iterrows():\n",
    "        if line.intersects(row['geometry']) and row['highway'] in separation_options:\n",
    "            return True\n",
    "\n",
    "    possible_matches_lists = filtered_roads_lists.iloc[list(filtered_roads_lists_sindex.intersection(line.bounds))]\n",
    "    for index, row in possible_matches_lists.iterrows():\n",
    "        if line.intersects(row['geometry']):\n",
    "            list_highway = row['highway']\n",
    "            if any(x in separation_options for x in list_highway):\n",
    "                return True\n",
    "    \n",
    "    # Check intersection with filtered rivers\n",
    "    possible_matches_rivers = filtered_rivers.iloc[list(filtered_rivers_sindex.intersection(line.bounds))]\n",
    "    for index, row in possible_matches_rivers.iterrows():\n",
    "        if line.intersects(row['geometry']):\n",
    "            return True\n",
    "\n",
    "    # Check intersection with filtered streams\n",
    "    possible_matches_streams = filtered_streams.iloc[list(filtered_streams_sindex.intersection(line.bounds))]\n",
    "    for index, row in possible_matches_streams.iterrows():\n",
    "        if line.intersects(row['geometry']):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD POINTS TO NX GRAPH\n",
    "# Function to add only points to the networkX graph\n",
    "# The other functions focuses on adding polygons, this function just iterates and adds points\n",
    "\n",
    "def add_points_to_graph(graph, graph_to_add):\n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        if 'geometry' in node_data and node_data['geometry'].geom_type in ['Point']:\n",
    "            if node_data['amenity'] == 'transportation':\n",
    "                isTranspo = True\n",
    "            else:\n",
    "                isTranspo = False\n",
    "                \n",
    "                graph_to_add.add_node(node_key, geometry=node_data['geometry'] name=node_data['name'], lat=node_data['lat'], amenity=node_data['amenity'],\n",
    "                                lon=node_data['lon'], isTranspo=isTranspo)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL 1 - COMBINE AMENITIES BY POLYGON\n",
    "# Creates a copy of a graph and connects non-contiguous and non-overlapping shapes instead of merging\n",
    "\n",
    "def combine_amenities_by_polygon(graph, max_distance, max_perimeter):\n",
    "    combined_graph = nx.Graph()\n",
    "    list_to_merge = []\n",
    "    idx = rtree_index.Index()\n",
    "    \n",
    "    # Iterates through each polygon and then enlarges and gets the intersecting ones for easier iteration later\n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        # Ensure that the bounding box coordinates are passed as a tuple\n",
    "        if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            enlarged_polygon = node_data['geometry'].buffer(meters_to_degrees(max_distance))\n",
    "            bounds = enlarged_polygon.bounds\n",
    "            bounds_float = tuple(float(coord) for coord in bounds)\n",
    "            numeric_key = int(node_key.split('_')[1])\n",
    "            idx.insert(numeric_key, bounds_float)\n",
    "    \n",
    "    #Iterating through each polygon\n",
    "    for node_key, node_data in list(graph.nodes.items()):\n",
    "        if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            nodes_to_merge = []\n",
    "            \n",
    "            #Check first if it is already in a list of polygons to be merged\n",
    "            for merge_list in list_to_merge:\n",
    "                if node_key in merge_list:\n",
    "                    nodes_to_merge = merge_list\n",
    "                    break\n",
    "            \n",
    "            # If this is a new node that is not part of any list, add itself to the list for merging later\n",
    "            if not nodes_to_merge:\n",
    "                nodes_to_merge.append(node_key)\n",
    "            \n",
    "            # Distance \n",
    "            total_distance = 0 # This is to calculate the total distance\n",
    "            combined_node = graph.nodes[node_key]['geometry']\n",
    "            \n",
    "            # Then iterate through other polygons that intersect that polygon based on bounds\n",
    "            for other_node_key in idx.intersection(node_data['geometry'].bounds):\n",
    "                formatted_key = f\"polygon_{other_node_key}\"\n",
    "                other_node_data = graph.nodes[formatted_key]\n",
    "                if 'geometry' in other_node_data and other_node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                    # Check if its not the same node, it is the same amenity, and is not already in the list to merge\n",
    "                    if node_key != formatted_key and node_data['amenity'] == other_node_data['amenity']:\n",
    "                        distance = degrees_to_meters(node_data['geometry'].distance(other_node_data['geometry']))\n",
    "\n",
    "                        if distance <= max_distance:\n",
    "                            line_between_centroids = LineString([node_data['geometry'].centroid, other_node_data['geometry'].centroid])\n",
    "                            amenities_intersecting = any(graph.nodes[amenity_key]['geometry'].intersects(line_between_centroids) for amenity_key in graph.nodes if amenity_key != node_key and amenity_key != formatted_key and graph.nodes[amenity_key]['amenity'] != node_data['amenity'])\n",
    "                            \n",
    "                            # Check if it does not exceed the max perimeter\n",
    "                            combined_node = shapely.ops.unary_union([combined_node, graph.nodes[formatted_key]['geometry']])\n",
    "                            total_distance += degrees_to_meters(combined_node.length)\n",
    "                            \n",
    "                            if not amenities_intersecting and total_distance < max_perimeter and not find_intersecting_features(line_between_centroids):\n",
    "                                nodes_to_merge.append(formatted_key)\n",
    "            \n",
    "            if nodes_to_merge not in list_to_merge:\n",
    "                list_to_merge.append(nodes_to_merge) # Add to the list to merge the polygons later\n",
    "                \n",
    "            \n",
    "                \n",
    "    temp_graph = to_graph(list_to_merge)\n",
    "    lists = graph_to_list(temp_graph)\n",
    "\n",
    "\n",
    "    # Now we will finally connect all polygons in the list\n",
    "    for merge_list in lists:\n",
    "        first = True\n",
    "        for node_key in merge_list:\n",
    "            if first:\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "                combined_node_amenity = graph.nodes[node_key]['amenity']\n",
    "                combined_node_key = node_key\n",
    "                combined_node_geometry = graph.nodes[node_key]['geometry']\n",
    "                combined_node_name = graph.nodes[node_key]['name']\n",
    "                combined_node_lat = graph.nodes[node_key]['lat']\n",
    "                combined_node_lon = graph.nodes[node_key]['lon']\n",
    "                combined_node_points = graph.nodes[node_key]['amenity_points']\n",
    "                first = False\n",
    "            else:\n",
    "                combined_node = shapely.ops.unary_union([combined_node_geometry, graph.nodes[node_key]['geometry']])\n",
    "                combined_node_geometry = combined_node\n",
    "                combined_node_name = combine_names(combined_node_name, graph.nodes[node_key].get('name'))\n",
    "                combined_node_lat = combined_node_geometry.centroid.x\n",
    "                combined_node_lon = combined_node_geometry.centroid.x\n",
    "                combined_node_points += graph.nodes[node_key].get('amenity_points', 0)\n",
    "                \n",
    "        combined_graph.add_node(combined_node_key, geometry=combined_node_geometry, name=combined_node_name, lat=combined_node_lat, amenity=combined_node_amenity,\n",
    "                                lon=combined_node_lon, amenity_points=combined_node_points)\n",
    "\n",
    "    return combined_graph\n",
    "\n",
    "# TEMPORARY SOLUTION FOR NULL NAMES\n",
    "def combine_names(name1, name2):\n",
    "    # Combine names ensuring that no null values are included\n",
    "    if isinstance(name1, str) and isinstance(name2, str):\n",
    "        return f\"{name1}, {name2}\"\n",
    "    elif isinstance(name1, str):\n",
    "        return name1\n",
    "    elif isinstance(name2, str):\n",
    "        return name2\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY\n",
    "\n",
    "def combine_residential(graph, max_distance, max_perimeter):\n",
    "    combined_graph = nx.Graph()\n",
    "    list_to_merge = []\n",
    "    idx = rtree_index.Index()\n",
    "    \n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        if 'amenity' in node_data and node_data['amenity'] == 'residential areas':\n",
    "            if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                enlarged_polygon = node_data['geometry'].buffer(meters_to_degrees(max_distance))\n",
    "                bounds = enlarged_polygon.bounds\n",
    "                bounds_float = tuple(float(coord) for coord in bounds)\n",
    "                numeric_key = int(node_key.split('_')[1])\n",
    "                idx.insert(numeric_key, bounds_float)\n",
    "\n",
    "    loop_count = 1\n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        print(\"Polgon Count: \", loop_count, \" / \", total_residential)\n",
    "        \n",
    "        if 'amenity' in node_data and node_data['amenity'] == 'residential areas':\n",
    "            if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                nodes_to_merge = []\n",
    "            \n",
    "                #Check first if it is already in a list of polygons to be merged\n",
    "                for merge_list in list_to_merge:\n",
    "                    if node_key in merge_list:\n",
    "                        nodes_to_merge = merge_list\n",
    "                        break\n",
    "            \n",
    "                # If this is a new node that is not part of any list, add itself to the list for merging later\n",
    "                if not nodes_to_merge:\n",
    "                    nodes_to_merge.append(node_key)\n",
    "            \n",
    "                # Distance \n",
    "                total_distance = 0 # This is to calculate the total distance\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "\n",
    "                sub_poly_count = 1\n",
    "                for other_node_key in idx.intersection(node_data['geometry'].bounds):\n",
    "                    print(f\"Currently checking {sub_poly_count}\")\n",
    "                    \n",
    "                    formatted_key = f\"polygon_{other_node_key}\"\n",
    "                    other_node_data = graph.nodes[formatted_key]\n",
    "                    if 'amenity' in other_node_data and other_node_data['amenity'] == 'residential areas':\n",
    "                        if 'geometry' in other_node_data and other_node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                            # Check if its not the same node, it is the same amenity, and is not already in the list to merge\n",
    "                            if node_key != formatted_key and node_data['amenity'] == other_node_data['amenity']:\n",
    "                                distance = degrees_to_meters(node_data['geometry'].distance(other_node_data['geometry']))\n",
    "                                line_between_centroids = LineString([node_data['geometry'].centroid, other_node_data['geometry'].centroid])\n",
    "                                amenities_intersecting = any(graph.nodes[amenity_key]['geometry'].intersects(line_between_centroids) for amenity_key in graph.nodes if amenity_key != node_key and amenity_key != formatted_key and graph.nodes[amenity_key]['amenity'] != node_data['amenity'])\n",
    "                        \n",
    "                                # Check if it does not exceed the max perimeter\n",
    "                                combined_node = shapely.ops.unary_union([combined_node, graph.nodes[formatted_key]['geometry']])\n",
    "                                total_distance += degrees_to_meters(combined_node.length)\n",
    "                        \n",
    "                                if not amenities_intersecting and total_distance < max_perimeter and not find_intersecting_features(line_between_centroids):\n",
    "                                    nodes_to_merge.append(formatted_key)\n",
    "                    sub_poly_count += 1\n",
    "            \n",
    "                if nodes_to_merge not in list_to_merge:\n",
    "                    list_to_merge.append(nodes_to_merge) # Add to the list to merge the polygons later\n",
    "                    \n",
    "        loop_count += 1\n",
    "    temp_graph = to_graph(list_to_merge)\n",
    "    lists = graph_to_list(temp_graph)\n",
    "\n",
    "\n",
    "    # Now we will finally connect all polygons in the list\n",
    "    for merge_list in lists:\n",
    "        first = True\n",
    "        for node_key in merge_list:\n",
    "            if first:\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "                combined_node_amenity = graph.nodes[node_key]['amenity']\n",
    "                combined_node_key = node_key\n",
    "                combined_node_geometry = graph.nodes[node_key]['geometry']\n",
    "                combined_node_name = graph.nodes[node_key]['name']\n",
    "                combined_node_lat = graph.nodes[node_key]['lat']\n",
    "                combined_node_lon = graph.nodes[node_key]['lon']\n",
    "                combined_node_points = graph.nodes[node_key]['amenity_points']\n",
    "                first = False\n",
    "            else:\n",
    "                combined_node = shapely.ops.unary_union([combined_node_geometry, graph.nodes[node_key]['geometry']])\n",
    "                combined_node_geometry = combined_node\n",
    "                combined_node_name = combine_names(combined_node_name, graph.nodes[node_key].get('name'))\n",
    "                combined_node_lat = combined_node_geometry.centroid.x\n",
    "                combined_node_lon = combined_node_geometry.centroid.x\n",
    "                combined_node_points += graph.nodes[node_key].get('amenity_points', 0)\n",
    "        \n",
    "        print(\"Added\")\n",
    "        combined_graph.add_node(combined_node_key, geometry=combined_node_geometry, name=combined_node_name, lat=combined_node_lat, amenity=combined_node_amenity,\n",
    "                                lon=combined_node_lon, amenity_points=combined_node_points)\n",
    "\n",
    "    return combined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY\n",
    "def combine_small_residential(graph, max_distance, max_perimeter):\n",
    "    combined_graph = nx.Graph()\n",
    "    list_to_merge = []\n",
    "    idx = rtree_index.Index()\n",
    "    \n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        if 'amenity' in node_data and node_data['amenity'] == 'residential areas':\n",
    "            if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                enlarged_polygon = node_data['geometry'].buffer(meters_to_degrees(20))\n",
    "                bounds = enlarged_polygon.bounds\n",
    "                bounds_float = tuple(float(coord) for coord in bounds)\n",
    "                numeric_key = int(node_key.split('_')[1])\n",
    "                idx.insert(numeric_key, bounds_float)\n",
    "\n",
    "    loop_count = 1\n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        print(\"Polgon Count: \", loop_count, \" / \", total_residential)\n",
    "        \n",
    "        if 'amenity' in node_data and node_data['amenity'] == 'residential areas':\n",
    "            if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                nodes_to_merge = []\n",
    "            \n",
    "                #Check first if it is already in a list of polygons to be merged\n",
    "                for merge_list in list_to_merge:\n",
    "                    if node_key in merge_list:\n",
    "                        nodes_to_merge = merge_list\n",
    "                        break\n",
    "            \n",
    "                # If this is a new node that is not part of any list, add itself to the list for merging later\n",
    "                if not nodes_to_merge:\n",
    "                    nodes_to_merge.append(node_key)\n",
    "            \n",
    "                # Distance \n",
    "                total_distance = 0 # This is to calculate the total distance\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "\n",
    "                sub_poly_count = 1\n",
    "                for other_node_key in idx.intersection(node_data['geometry'].bounds):\n",
    "                    print(f\"Polygon {loop_count}: Currently checking {sub_poly_count}\")\n",
    "                    \n",
    "                    formatted_key = f\"polygon_{other_node_key}\"\n",
    "                    other_node_data = graph.nodes[formatted_key]\n",
    "                    if 'amenity' in other_node_data and other_node_data['amenity'] == 'residential areas':\n",
    "                        if 'geometry' in other_node_data and other_node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                            # Check if its not the same node, it is the same amenity, and is not already in the list to merge\n",
    "                            if node_key != formatted_key and node_data['amenity'] == other_node_data['amenity']:\n",
    "                                distance = degrees_to_meters(node_data['geometry'].distance(other_node_data['geometry']))\n",
    "                                line_between_centroids = LineString([node_data['geometry'].centroid, other_node_data['geometry'].centroid])\n",
    "                                amenities_intersecting = any(graph.nodes[amenity_key]['geometry'].intersects(line_between_centroids) for amenity_key in graph.nodes if amenity_key != node_key and amenity_key != formatted_key and graph.nodes[amenity_key]['amenity'] != node_data['amenity'])\n",
    "                        \n",
    "                                # Check if it does not exceed the max perimeter\n",
    "                                combined_node = shapely.ops.unary_union([combined_node, graph.nodes[formatted_key]['geometry']])\n",
    "                                total_distance += degrees_to_meters(combined_node.length)\n",
    "                        \n",
    "                                if not amenities_intersecting and total_distance < max_perimeter and not find_intersecting_features(line_between_centroids):\n",
    "                                    nodes_to_merge.append(formatted_key)\n",
    "                    sub_poly_count += 1\n",
    "            \n",
    "                if nodes_to_merge not in list_to_merge:\n",
    "                    list_to_merge.append(nodes_to_merge) # Add to the list to merge the polygons later\n",
    "                    \n",
    "        loop_count += 1\n",
    "        \n",
    "    temp_graph = to_graph(list_to_merge)\n",
    "    lists = graph_to_list(temp_graph)\n",
    "\n",
    "\n",
    "    # Now we will finally connect all polygons in the list\n",
    "    for merge_list in lists:\n",
    "        first = True\n",
    "        for node_key in merge_list:\n",
    "            if first:\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "                combined_node_amenity = graph.nodes[node_key]['amenity']\n",
    "                combined_node_key = node_key\n",
    "                combined_node_geometry = graph.nodes[node_key]['geometry']\n",
    "                combined_node_name = graph.nodes[node_key]['name']\n",
    "                combined_node_lat = graph.nodes[node_key]['lat']\n",
    "                combined_node_lon = graph.nodes[node_key]['lon']\n",
    "                combined_node_points = graph.nodes[node_key]['amenity_points']\n",
    "                first = False\n",
    "            else:\n",
    "                combined_node = shapely.ops.unary_union([combined_node_geometry, graph.nodes[node_key]['geometry']])\n",
    "                combined_node_geometry = combined_node\n",
    "                combined_node_name = combine_names(combined_node_name, graph.nodes[node_key].get('name'))\n",
    "                combined_node_lat = combined_node_geometry.centroid.x\n",
    "                combined_node_lon = combined_node_geometry.centroid.x\n",
    "                combined_node_points += graph.nodes[node_key].get('amenity_points', 0)\n",
    "        \n",
    "        print(\"Added\")\n",
    "        combined_graph.add_node(combined_node_key, geometry=combined_node_geometry, name=combined_node_name, lat=combined_node_lat, amenity=combined_node_amenity,\n",
    "                                lon=combined_node_lon, amenity_points=combined_node_points)\n",
    "\n",
    "    return combined_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To merge duplicates in lists\n",
    "def to_graph(nodes):\n",
    "    G = nx.Graph()\n",
    "    for part in nodes:\n",
    "        G.add_nodes_from(part)\n",
    "        G.add_edges_from(to_edges(part))\n",
    "    return G\n",
    "\n",
    "def to_edges(nodes):\n",
    "    it = iter(nodes)\n",
    "    last = next(it)\n",
    "\n",
    "    for current in it:\n",
    "        yield last, current\n",
    "        last = current\n",
    "        \n",
    "def graph_to_list(G):\n",
    "    connected_components = nx.connected_components(G)\n",
    "    lists = [list(component) for component in connected_components]\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Function to plot/visualize main graph network on the map\n",
    "def plot_network_on_map(amenities_network, initial_location=[0, 0], zoom_start=10):\n",
    "    # Create a map centered at the initial location\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=zoom_start, tiles='openstreetmap')\n",
    "    \n",
    "    #Colours for Visualization\n",
    "    amenity_colors = {\n",
    "        'education': 'green',\n",
    "        'finance': 'blue',\n",
    "        'government offices': 'red',\n",
    "        'grocery': 'orange',\n",
    "        'health': 'magenta',\n",
    "        'malls': 'yellow',\n",
    "        'residential areas': 'brown',\n",
    "        'security': 'gray',\n",
    "        'transportation': 'lightblue',\n",
    "        'others': 'black'\n",
    "    }\n",
    "\n",
    "    # Iterate over the nodes in the network\n",
    "    for node, data in amenities_network.nodes(data=True):\n",
    "        # Check if the node has a geometry attribute\n",
    "        if 'geometry' in data:\n",
    "            # Get the geometry of the node\n",
    "            geometry = data['geometry']\n",
    "\n",
    "            # Check the geometry type and plot accordingly\n",
    "            if geometry.geom_type == 'Point':\n",
    "                # Plot a marker for points    \n",
    "                #folium.Marker(location=[geometry.y, geometry.x], popup=f\"{data['name']}\").add_to(m)\n",
    "                continue\n",
    "            elif geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                # Plot polygons or multipolygons\n",
    "                color = amenity_colors[data.get('amenity')]\n",
    "                if geometry.geom_type == 'Polygon':\n",
    "                    polygons = [geometry]\n",
    "                else:\n",
    "                    polygons = geometry.geoms\n",
    "\n",
    "                for polygon in polygons:\n",
    "                    coordinates = []\n",
    "                    for point in polygon.exterior.coords:\n",
    "                        coordinates.append([point[1], point[0]])\n",
    "                    folium.Polygon(locations=coordinates, fill=True, color=color, fill_opacity=0.4).add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL 2 - Function to connect zones in a network\n",
    "def create_zone_network(graph, max_distance):\n",
    "    connect_graph = nx.Graph()\n",
    "    network_id = 1\n",
    "    list_to_connect = []\n",
    "    idx = rtree_index.Index()\n",
    "    \n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        enlarged_polygon = node_data['geometry'].buffer(meters_to_degrees(max_distance))\n",
    "        bounds = enlarged_polygon.bounds\n",
    "        bounds_float = tuple(float(coord) for coord in bounds)\n",
    "        numeric_key = int(node_key.split('_')[1])\n",
    "        idx.insert(numeric_key, bounds_float)\n",
    "    \n",
    "    for node_key, node_data in list(graph.nodes.items()):\n",
    "        if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "            connect_nodes = []\n",
    "            \n",
    "            #Check first if it is already in a list of polygons to be connected\n",
    "            for connect_list in list_to_connect:\n",
    "                if node_key in connect_list:\n",
    "                    connect_nodes = connect_list\n",
    "                    break\n",
    "            \n",
    "            # If this is a new node that is not part of any list, add itself to the list for merging later\n",
    "            if not connect_nodes:\n",
    "                connect_nodes.append(node_key)\n",
    "                \n",
    "            # If this is not a residential area that is its own zone\n",
    "            if node_key not in pop_graph or not pop_graph.nodes[node_key]['is_a_zone']:\n",
    "                for other_node_key in idx.intersection(node_data['geometry'].bounds):\n",
    "                    formatted_key = f\"polygon_{other_node_key}\"\n",
    "                    other_node_data = graph.nodes[formatted_key]\n",
    "                    if 'geometry' in other_node_data and other_node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                        if node_key != formatted_key:\n",
    "\n",
    "                            distance = degrees_to_meters(node_data['geometry'].distance(other_node_data['geometry']))\n",
    "\n",
    "                            # Check if they are within distance of each other\n",
    "                            if distance <= max_distance:\n",
    "                                line_between_centroids = LineString([node_data['geometry'].centroid, other_node_data['geometry'].centroid])\n",
    "                                if not find_intersecting_features(line_between_centroids):\n",
    "                                    if formatted_key not in pop_graph or not pop_graph.nodes[formatted_key]['is_a_zone']:\n",
    "                                        connect_nodes.append(formatted_key)\n",
    "                                \n",
    "            if connect_nodes not in list_to_connect:\n",
    "                list_to_connect.append(connect_nodes) # Add to the list to merge the polygons later\n",
    "                \n",
    "    temp_graph = to_graph(list_to_connect)\n",
    "    lists = graph_to_list(temp_graph)\n",
    "\n",
    "    # Now we will finally connect all polygons in the list\n",
    "    for merge_list in lists:\n",
    "        first = True\n",
    "        for node_key in merge_list:\n",
    "            if first:\n",
    "                combined_node = graph.nodes[node_key]['geometry']\n",
    "                combined_node_amenity = graph.nodes[node_key]['amenity']\n",
    "                combined_node_key = node_key\n",
    "                combined_node_geometry = graph.nodes[node_key]['geometry']\n",
    "                combined_node_name = graph.nodes[node_key]['name']\n",
    "                combined_node_lat = graph.nodes[node_key]['lat']\n",
    "                combined_node_lon = graph.nodes[node_key]['lon']\n",
    "                combined_node_points = graph.nodes[node_key]['amenity_points']\n",
    "                first = False\n",
    "            else:\n",
    "                combined_node = shapely.ops.unary_union([combined_node_geometry, graph.nodes[node_key]['geometry']])\n",
    "                combined_node_geometry = combined_node\n",
    "                combined_node_name = combine_names(combined_node_name, graph.nodes[node_key].get('name'))\n",
    "                combined_node_lat = combined_node_geometry.centroid.x\n",
    "                combined_node_lon = combined_node_geometry.centroid.x\n",
    "                combined_node_points += graph.nodes[node_key].get('amenity_points', 0)\n",
    "                \n",
    "        network_id += 1\n",
    "        connect_graph.add_node(combined_node_key, geometry=combined_node_geometry, name=combined_node_name, lat=combined_node_lat, amenity=combined_node_amenity,\n",
    "                                lon=combined_node_lon, amenity_points=combined_node_points, network_id=network_id)\n",
    "\n",
    "    return connect_graph\n",
    "\n",
    "# TEMPORARY SOLUTION FOR NULL NAMES\n",
    "def combine_names(name1, name2):\n",
    "    # Combine names ensuring that no null values are included\n",
    "    if isinstance(name1, str) and isinstance(name2, str):\n",
    "        return f\"{name1}, {name2}\"\n",
    "    elif isinstance(name1, str):\n",
    "        return name1\n",
    "    elif isinstance(name2, str):\n",
    "        return name2\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Function to plot/visualize connected zones on the map\n",
    "import random\n",
    "\n",
    "# This is to better visualize the networks\n",
    "def plot_connected_zones_network_on_map(graph, initial_location=[0, 0], zoom_start=10):\n",
    "    # Create a map centered at the initial location\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=zoom_start, tiles='openstreetmap')\n",
    "    \n",
    "    #Colours for Visualization\n",
    "    colors = [\n",
    "    \"Red\", \"Green\", \"Blue\", \"Yellow\", \"Orange\", \"Purple\", \"Cyan\", \"Magenta\", \"Maroon\",\n",
    "    \"Olive\", \"Lime\", \"Teal\", \"Navy\", \"Aqua\", \"Fuchsia\", \"Coral\", \"Indigo\", \"Violet\"]\n",
    "    \n",
    "    color_map = {}\n",
    "\n",
    "    # Iterate over the nodes in the network\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        # Check if the node has a geometry attribute\n",
    "        if 'geometry' in data:\n",
    "            # Get the geometry of the node\n",
    "            geometry = data['geometry']\n",
    "\n",
    "            # Check the geometry type and plot accordingly\n",
    "            if geometry.geom_type == 'Point':\n",
    "                # Plot a marker for points    \n",
    "                #folium.Marker(location=[geometry.y, geometry.x], popup=f\"{data['name']}\").add_to(m)\n",
    "                continue\n",
    "            elif geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                \n",
    "                network_id = data[\"network_id\"]\n",
    "                \n",
    "                if network_id not in color_map:\n",
    "                    color = random.choice(colors)\n",
    "                    color_map[network_id] = color\n",
    "                else:\n",
    "                    color = color_map[network_id]\n",
    "                \n",
    "                if geometry.geom_type == 'Polygon':\n",
    "                    polygons = [geometry]\n",
    "                else:\n",
    "                    polygons = geometry.geoms\n",
    "\n",
    "                for polygon in polygons:\n",
    "                    coordinates = []\n",
    "                    for point in polygon.exterior.coords:\n",
    "                        coordinates.append([point[1], point[0]])\n",
    "                    folium.Polygon(locations=coordinates, fill=True, color=color, fill_opacity=0.4).add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to visualize the stops\n",
    "def plot_stops_on_map(network_map, stops, initial_location=[0, 0], zoom_start=10):\n",
    "    # Iterate over the nodes in the network\n",
    "    for stop in stops:\n",
    "        folium.Marker(location=[stop.lat, stop.long], popup=f\"{stop.isTranspo}\").add_to(network_map)\n",
    "        \n",
    "    return network_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check population density - To be used for the zone connection\n",
    "# Uses the combined graph\n",
    "# Formula: Population Density = Total Population / Total Area\n",
    "\n",
    "def check_residential_population_density(graph, threshold):\n",
    "    # Create an R-tree index for efficient spatial querying\n",
    "    idx = rtree_index.Index()\n",
    "    \n",
    "    # Populate the R-tree index with points\n",
    "    for index, row in manila_population_gdf.iterrows():\n",
    "        idx.insert(index, (row['longitude'], row['latitude'], row['longitude'], row['latitude']))\n",
    "    \n",
    "    pop_graph = nx.Graph()\n",
    "    \n",
    "    for node_key, node_data in graph.nodes.items():\n",
    "        # Check if its a polygon and is a residential area\n",
    "        if 'geometry' in node_data and node_data['geometry'].geom_type in ['Polygon', 'MultiPolygon'] and node_data['amenity'] == \"residential areas\":\n",
    "            total_pop = 0\n",
    "            \n",
    "            # Query the R-tree index to find points within the polygon\n",
    "            for point_idx in idx.intersection(node_data['geometry'].bounds):\n",
    "                point = manila_population_gdf.loc[point_idx]\n",
    "                if Point(point['longitude'], point['latitude']).within(node_data['geometry']):\n",
    "                    total_pop += point['phl_general_2020']  # Add the density\n",
    "            \n",
    "            density = total_pop / node_data['geometry'].area\n",
    "            \n",
    "            if density > threshold:\n",
    "                node_data[\"is_a_zone\"] = True\n",
    "            else:\n",
    "                node_data[\"is_a_zone\"] = False\n",
    "            \n",
    "            pop_graph.add_node(node_key, density=density, **node_data)\n",
    "    return pop_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 - Function to plot/visualize residential areas based on population density on the map\n",
    "# This is to better visualize which residential areas can become zones\n",
    "def plot_population_zones_map(graph, initial_location=[0, 0], zoom_start=10):\n",
    "    # Create a map centered at the initial location\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=zoom_start, tiles='openstreetmap')\n",
    "\n",
    "    # Iterate over the nodes in the network\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        # Check if the node has a geometry attribute\n",
    "        if 'geometry' in data:\n",
    "            # Get the geometry of the node\n",
    "            geometry = data['geometry']\n",
    "\n",
    "            # Check the geometry type and plot accordingly\n",
    "            if geometry.geom_type == 'Point':\n",
    "                # Plot a marker for points    \n",
    "                #folium.Marker(location=[geometry.y, geometry.x], popup=f\"{data['name']}\").add_to(m)\n",
    "                continue\n",
    "            elif geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                if geometry.geom_type == 'Polygon':\n",
    "                    polygons = [geometry]\n",
    "                else:\n",
    "                    polygons = geometry.geoms\n",
    "\n",
    "                for polygon in polygons:\n",
    "                    coordinates = []\n",
    "                    for point in polygon.exterior.coords:\n",
    "                        coordinates.append([point[1], point[0]])\n",
    "                    \n",
    "                    if (data['is_a_zone']):\n",
    "                        folium.Polygon(locations=coordinates, fill=True, color=\"green\", fill_opacity=0.4).add_to(m)\n",
    "                    else:\n",
    "                        folium.Polygon(locations=coordinates, fill=True, color=\"red\", fill_opacity=0.4).add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing of Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING STOPS\n",
    "# It should return a list of coordinates/nodes for stop and a graph of stops\n",
    "# if residential area, check if the population density\n",
    "\n",
    "# Global Variables used:\n",
    "# list_of_stops - List of stops\n",
    "# graph_of_stops - graph of all stops placed\n",
    "# CITY_GPRAH - graph of the city road networks\n",
    "import random\n",
    "\n",
    "def place_stops_on_roads(amenity_graph):\n",
    "    # graph_of_stops = nx.Graph()\n",
    "    \n",
    "    for node_key, node_data in amenity_graph.nodes(data=True):\n",
    "        # All tranportation points are automatically stops\n",
    "        if node_data['geometry'] == 'Point' and node_data['amenity'] == 'transportation':\n",
    "            nearest_node = ox.distance.nearest_nodes(CITY_GRAPH, node_data['lon'], node_data['lat'])\n",
    "            \n",
    "            # If there is an existing node in the main graph, then add it to the list and the stop graph\n",
    "            if nearest_node is not None:\n",
    "                list_of_stops.append(stopCandidate(CITY_GRAPH.nodes[nearest_node]['y'], CITY_GRAPH.nodes[nearest_node]['x'], True))\n",
    "                lon = CITY_GRAPH.nodes[nearest_node]['x']\n",
    "                lat = CITY_GRAPH.nodes[nearest_node]['y']\n",
    "                isTranspo = True\n",
    "                graph_of_stops.add_node(nearest_node, lon=lon, lat=lat, isTranspo=isTranspo)\n",
    "        \n",
    "        else:\n",
    "            # Calculate the number of stops based on node size and population density\n",
    "            num_stops = calculate_num_stops(node_key, node_data)\n",
    "            \n",
    "            buffer_poly = node_data['geometry'].buffer(meters_to_degrees(30))\n",
    "            # Get the roads surrounding and inside the node polygons\n",
    "            relevant_edges = get_relevant_edges(buffer_poly)\n",
    "            \n",
    "            # Place stops randomly on these roads\n",
    "            place_stops_along_edges(relevant_edges, buffer_poly, num_stops)\n",
    "\n",
    "def calculate_num_stops(node_key, node_data):\n",
    "    # Example calculation based on node size and population density\n",
    "    node_size = degrees_to_meters(node_data['geometry'].area) # Size of the node polygon\n",
    "    # Adjust factors and formula as needed\n",
    "    num_stops = 0\n",
    "    \n",
    "    if node_key in pop_graph:\n",
    "        pop_density = pop_graph.nodes[node_key]['density']\n",
    "        num_stops = node_size * pop_density / 10000000  # Adjust this factor as needed\n",
    "        \n",
    "        if num_stops < 1:\n",
    "            num_stops = 1\n",
    "        elif num_stops > 3:\n",
    "            num_stops = 3\n",
    "    else:\n",
    "        list_sum = len(node_data['amenity_points'])\n",
    "\n",
    "        if list_sum > 0:\n",
    "            num_stops = 1\n",
    "        \n",
    "    return int(num_stops)\n",
    "\n",
    "\n",
    "# Create spatial index\n",
    "filtered_roads_strings_sindex = filtered_roads_strings.sindex\n",
    "filtered_roads_lists_sindex = filtered_roads_lists.sindex\n",
    "def get_relevant_edges(polygon):\n",
    "    relevant_edges = []\n",
    "    \n",
    "    # Check intersection with filtered roads\n",
    "    possible_matches_roads = filtered_roads_strings.iloc[list(filtered_roads_strings_sindex.intersection(polygon.bounds))]\n",
    "    for index, row in possible_matches_roads.iterrows():\n",
    "        if polygon.intersects(row['geometry']) and row['highway'] in ['primary', 'secondary', 'tertiary', 'residential']:\n",
    "            relevant_edges.append(row)\n",
    "\n",
    "    possible_matches_lists = filtered_roads_lists.iloc[list(filtered_roads_lists_sindex.intersection(polygon.bounds))]\n",
    "    for index, row in possible_matches_lists.iterrows():\n",
    "        if polygon.intersects(row['geometry']):\n",
    "            list_highway = row['highway']\n",
    "            if any(x in ['primary', 'secondary', 'tertiary', 'residential'] for x in list_highway):\n",
    "                relevant_edges.append(row)\n",
    "    return relevant_edges\n",
    "\n",
    "def place_stops_along_edges(edges, polygon, num_stops):\n",
    "    # Place stops randomly along the edges within the polygon\n",
    "    \n",
    "    if len(edges) > 0:\n",
    "        for _ in range(num_stops):\n",
    "            edge = random.choice(edges)\n",
    "            # Calculate the intersection between the edge and the polygon\n",
    "            intersecting_line = edge['geometry'].intersection(polygon)\n",
    "            if intersecting_line.is_empty:\n",
    "                continue\n",
    "\n",
    "            # Calculate the length of the intersecting part of the edge\n",
    "            intersecting_length = intersecting_line.length\n",
    "\n",
    "            # Generate a random position along the intersecting part of the edge\n",
    "            random_position = random.uniform(0, intersecting_length)\n",
    "\n",
    "            # Calculate the coordinate along the edge at the random position\n",
    "            stop_location = calculate_coordinate_along_edge(intersecting_line, random_position)\n",
    "            #print(\"Stop placed at:\", stop_location)\n",
    "            \n",
    "            nearest_node = ox.distance.nearest_nodes(CITY_GRAPH, stop_location[0], stop_location[1])\n",
    "            \n",
    "            # If there is an existing node in the main graph, then add it to the list and the stop graph\n",
    "            if nearest_node is not None:\n",
    "                list_of_stops.append(stopCandidate(CITY_GRAPH.nodes[nearest_node]['y'], CITY_GRAPH.nodes[nearest_node]['x'], False))\n",
    "                lon = CITY_GRAPH.nodes[nearest_node]['x']\n",
    "                lat = CITY_GRAPH.nodes[nearest_node]['y']\n",
    "                isTranspo = False\n",
    "                graph_of_stops.add_node(nearest_node, lon=lon, lat=lat, isTranspo=isTranspo)\n",
    "        \n",
    "            \n",
    "\n",
    "def calculate_coordinate_along_edge(edge, position):\n",
    "    # Calculate the coordinate along the edge at the given position\n",
    "    point = edge.interpolate(position)\n",
    "    return point.x, point.y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the graph into a geojson for loading into QGIS\n",
    "\n",
    "def graph_to_geojson(graph, filename):\n",
    "    # Initialize an empty list to hold GeoJSON features\n",
    "    features = []\n",
    "\n",
    "    # Iterate over the nodes in the graph\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        # Check if the node has a geometry attribute\n",
    "        if 'geometry' in data:\n",
    "            # Convert the geometry to a GeoJSON-compatible format\n",
    "            geometry = shapely.geometry.shape(data['geometry'])\n",
    "            # Create a copy of the properties to check for NaN values\n",
    "            properties = data.copy()\n",
    "            # Remove the geometry from the properties\n",
    "            properties.pop('geometry', None)\n",
    "            # Check for NaN values in the properties\n",
    "            if all(not (isinstance(value, float) and np.isnan(value)) for value in properties.values()):\n",
    "                # Create a GeoJSON feature for the node\n",
    "                feature = geojson.Feature(geometry=geometry, properties=properties)\n",
    "                # Add the feature to the list\n",
    "                features.append(feature)\n",
    "\n",
    "    # Create a GeoJSON FeatureCollection\n",
    "    feature_collection = geojson.FeatureCollection(features)\n",
    "\n",
    "    # Return the GeoJSON FeatureCollection\n",
    "    return feature_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Route Network Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Route Network from connected routes\n",
    "\n",
    "# # Global Variables used:\n",
    "# # graph_of_stops - Graph of stops that will be used to create routes\n",
    "# def generate_route_network(stop_nodes, max_walking_dist):\n",
    "#     stop_node_coordinates = [[n.lat, n.long] for n in stop_nodes]\n",
    "#     stop_nodes_kd_tree = KDTree(stop_node_coordinates)\n",
    "#     next_nodes = [n for n in stop_nodes]\n",
    "#     enable_stop_nodes(next_nodes)\n",
    "#     route_network = []\n",
    "\n",
    "#     #Create empty graph\n",
    "#     new_graph = graph_of_stops.copy() # Copy of Manila\n",
    "\n",
    "    \n",
    "#     while not all_nodes_disabled(next_nodes) and len(next_nodes) != 0:\n",
    "#         selected_node = random.choice(next_nodes) # For the first node\n",
    "#         next_nodes.remove(selected_node)\n",
    "#         used_stops.append(selected_node)\n",
    "        \n",
    "#         route_network.append(generate_route(selected_node, next_nodes, stop_nodes_kd_tree, max_walking_dist, new_graph))\n",
    "\n",
    "#     return route_network, new_graph\n",
    "\n",
    "# # Generate route from stop nodes\n",
    "# def generate_route(source, next_nodes, stop_nodes_kd_tree, max_walking_dist, new_graph):\n",
    "#     route = []\n",
    "#     totalDistance = 0\n",
    "#     selected_node = source\n",
    "\n",
    "#     while not all_nodes_disabled(next_nodes) and totalDistance < MAX_DISTANCE:\n",
    "        \n",
    "#         #print(f\"Selected node is {selected_node.getLat()}, {selected_node.getLong()}\")\n",
    "#         disable_surrounding_nodes(next_nodes, stop_nodes_kd_tree, selected_node, max_walking_dist)\n",
    "#         enabled_nodes = [n for n in next_nodes if n.enabled]\n",
    "#         orig_node = ox.distance.nearest_nodes(new_graph, selected_node.getLong(), selected_node.getLat()) # Getting the node from the graph itself\n",
    "#         old_node = selected_node\n",
    "#         selected_node = get_enabled_node_with_highest_edge_probability(selected_node, enabled_nodes)\n",
    "        \n",
    "#         if (selected_node == None or selected_node == old_node):\n",
    "#             break\n",
    "        \n",
    "#         next_nodes.remove(selected_node)\n",
    "#         dest_node = ox.distance.nearest_nodes(new_graph, selected_node.getLong(), selected_node.getLat())\n",
    "        \n",
    "#         if orig_node is None or dest_node is None:\n",
    "#             print(\"Unable to find valid nodes. Please verify the start and end coordinates.\")\n",
    "#         elif not nx.has_path(graph, orig_node, dest_node):\n",
    "#             print(\"No valid path found between the start and end nodes.\")\n",
    "#         else:\n",
    "            \n",
    "#             shortest_route = nx.shortest_path(graph, orig_node, dest_node)\n",
    "#             distance_travelled = 0\n",
    "#             # Get the total distance from point A to point B\n",
    "#             for i in range(len(shortest_route)-1):\n",
    "#                 node_data = graph.nodes[shortest_route[i]]\n",
    "#                 next_node_data = graph.nodes[shortest_route[i+1]]\n",
    "                \n",
    "#                 distance_travelled += haversine(node_data['y'], node_data['x'], next_node_data['y'], next_node_data['x'])\n",
    "\n",
    "#             # Checks if it does not exceed the max distance\n",
    "#             if totalDistance + distance_travelled <= MAX_DISTANCE:\n",
    "#                 # Add the nodes to the nx graph\n",
    "#                 new_graph.add_edge(orig_node, dest_node, weight=distance_travelled)\n",
    "                \n",
    "#                 totalDistance += distance_travelled\n",
    "#                 used_stops.append(selected_node)\n",
    "#                 route.append(shortest_route)\n",
    "#             else:\n",
    "#                 break\n",
    "#     return route\n",
    "\n",
    "# # Disable surrounding nodes\n",
    "# def disable_surrounding_nodes(next_nodes, stop_nodes_kd_tree, source_node, max_distance):\n",
    "#     source = (source_node.getLat(), source_node.getLong())\n",
    "    \n",
    "#     for node in next_nodes:\n",
    "#         point = (node.getLat(), node.getLong())\n",
    "#         distance = geodesic(source, point).meters\n",
    "#         if distance <= max_distance:\n",
    "#             node.disable()\n",
    "        \n",
    "# def get_enabled_node_with_highest_edge_probability(source_node, enabled_nodes):\n",
    "#     highest_edge_prob = 0\n",
    "#     highest_edge_prob_node = None\n",
    "\n",
    "#     for n in enabled_nodes:\n",
    "#         edge_prob = get_edge_probability(source_node, n, len(enabled_nodes))\n",
    "#         if edge_prob > highest_edge_prob:\n",
    "#             highest_edge_prob = edge_prob\n",
    "#             highest_edge_prob_node = n\n",
    "\n",
    "#     return highest_edge_prob_node\n",
    "\n",
    "\n",
    "# def get_edge_probability(source, destination, normalization_factor):\n",
    "#     source_coord = [source.getLat(), source.getLong()]\n",
    "#     dest_coord = [destination.getLat(), destination.getLong()]\n",
    "#     return exp(-(euclidean(source_coord, dest_coord))) / float(normalization_factor)\n",
    "\n",
    "\n",
    "# def radius(stops):\n",
    "#     circles = []\n",
    "#     for stop in stops:\n",
    "#         stop_point = Point(stop[1], stop[0])  # Create a Point object from [lat, lon] coordinates\n",
    "#         circle = stop_point.buffer(radius / 111000)  # Buffer the Point to create a circle (assuming 1 degree is approximately 111000 meters)\n",
    "#         circles.append(circle)\n",
    "#     return circles\n",
    "\n",
    "# def enable_stop_nodes(stop_nodes):\n",
    "#     for n in stop_nodes:\n",
    "#         n.enable()\n",
    "\n",
    "# def all_nodes_disabled(stop_nodes):\n",
    "#     return get_num_disabled(stop_nodes) == len(stop_nodes)\n",
    "\n",
    "# def get_num_disabled(stop_nodes):\n",
    "#     return sum(1 for n in stop_nodes if not n.enabled)\n",
    "\n",
    "\n",
    "\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     # Use geopy's geodesic function to calculate the distance\n",
    "#     distance = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "#     return distance\n",
    "\n",
    "# # Markers for visualization purposes\n",
    "# def add_markers(used_stops):\n",
    "#     for stop in used_stops:\n",
    "#         #popup_text = f\"Name: {stop.name}<br>Type: {stop.a_type}<br>Coordinates: {stop.getLat()}, {stop.getLong()}\"\n",
    "#         folium.Marker(location=[stop.road_lat, stop.road_long]).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2 - Graph with the snapping function\n",
    "# Generate Route Network from connected routes\n",
    "\n",
    "# Global Variables used:\n",
    "# graph_of_stops - Graph of stops that will be used to create routes\n",
    "def generate_route_network(stop_nodes, max_walking_dist):\n",
    "    location_road_nodes = [node for node, data in CITY_GRAPH.nodes(data=True)]\n",
    "    overall_graph = nx.Graph()\n",
    "    \n",
    "    stop_node_coordinates = [[n.lat, n.long] for n in stop_nodes]\n",
    "    stop_nodes_kd_tree = KDTree(stop_node_coordinates)\n",
    "    next_nodes = [n for n in stop_nodes]\n",
    "    enable_stop_nodes(next_nodes)\n",
    "    route_network = []\n",
    "\n",
    "    while not all_nodes_disabled(next_nodes) and len(next_nodes) != 0:\n",
    "        selected_node = random.choice(next_nodes) # For the first node\n",
    "        next_nodes.remove(selected_node)\n",
    "        used_stops.append(selected_node)\n",
    "        route = generate_route(selected_node, next_nodes, stop_nodes_kd_tree, max_walking_dist, overall_graph)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #--\n",
    "        snapped_route = snap_route_to_road(location_road_nodes, route)\n",
    "        nx.set_edge_attributes(snapped_route, 'route_id', route_id)\n",
    "        route_id += 1\n",
    "        overall_graph = nx.compose(overall_graph, snapped_route)\n",
    "        \n",
    "        snapped_edges = list(snapped_route.edges(data='road_path', default=1))\n",
    "        snapped_route = connect_snapped_edges(snapped_edges)\n",
    "        #--\n",
    "        \n",
    "        route_network.append(route)\n",
    "\n",
    "    return route_network, overall_graph\n",
    "\n",
    "def connect_snapped_edges(snapped_edges):\n",
    "    connected_edge = []\n",
    "\n",
    "    while len(snapped_edges) > 0:\n",
    "        curr_edge = consecutive_connect([], snapped_edges.pop(0))\n",
    "\n",
    "        for e in snapped_edges:\n",
    "            consecutive_edge = consecutive_connect(curr_edge, e)\n",
    "            if consecutive_edge is not None:\n",
    "                curr_edge = consecutive_edge\n",
    "                snapped_edges.remove(e)\n",
    "\n",
    "        new_connected_edge = consecutive_connect(connected_edge, curr_edge)\n",
    "        connected_edge = new_connected_edge if new_connected_edge is not None else connected_edge\n",
    "\n",
    "    return connected_edge\n",
    "\n",
    "def consecutive_connect(e1, e2):\n",
    "    if len(e1) == 0:\n",
    "        return e2\n",
    "    elif len(e2) == 0:\n",
    "        return e1\n",
    "\n",
    "    if e1[-1] == e2[0]:\n",
    "        return e1 + e2\n",
    "    elif e2[-1] == e1[0]:\n",
    "        return e2 + e1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def snap_route_to_road(location_road_nodes, route_stop_nodes):\n",
    "    snapped_route = nx.Graph()\n",
    "    route_stops = [] # We need to store the list of stops as route_stop_nodes is only a list of ox.shortest_path results\n",
    "    \n",
    "    \n",
    "    # Directly add nodes based on node identifiers\n",
    "    for n in route_stop_nodes:\n",
    "        snapped_route.add_node(n[0], **graph.nodes[n[0]])\n",
    "        route_stops.append(n[0])\n",
    "        last_path = n # For the sake of getting the last stop of the last stop connection\n",
    "        \n",
    "    snapped_route.add_node(n[len(last_path)-1], **graph.nodes[n[len(last_path)-1]])\n",
    "    route_stops.append(n[len(last_path)-1])\n",
    "\n",
    "    # Add the shortest path for each consecutive node as an edge in the graph\n",
    "    for i in range(len(route_stops) - 1):\n",
    "        source_node = route_stops[i]\n",
    "        dest_node = route_stops[i + 1]\n",
    "        shortest_road_path = get_shortest_road_path(location_road_nodes, source_node, dest_node)\n",
    "        snapped_route.add_edge(source_node, dest_node, road_path=shortest_road_path)\n",
    "    return snapped_route\n",
    "\n",
    "def get_shortest_road_path(location_road_nodes, source_stop_node, dest_stop_node):\n",
    "    # Find the nearest nodes in the graph to the source and destination GeoPoints\n",
    "    closest_road_node_to_source = ox.distance.nearest_nodes(graph, graph.nodes[source_stop_node]['x'], graph.nodes[source_stop_node]['y'])\n",
    "    closest_road_node_to_dest = ox.distance.nearest_nodes(graph,  graph.nodes[dest_stop_node]['x'], graph.nodes[dest_stop_node]['y'])\n",
    "\n",
    "    if nx.has_path(graph, closest_road_node_to_source, closest_road_node_to_dest):\n",
    "        return nx.shortest_path(graph, closest_road_node_to_source, closest_road_node_to_dest)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Generate route from stop nodes\n",
    "def generate_route(source, next_nodes, stop_nodes_kd_tree, max_walking_dist, new_graph):\n",
    "    route = []\n",
    "    totalDistance = 0\n",
    "    selected_node = source\n",
    "\n",
    "    while not all_nodes_disabled(next_nodes) and totalDistance < MAX_DISTANCE:\n",
    "        \n",
    "        #print(f\"Selected node is {selected_node.getLat()}, {selected_node.getLong()}\")\n",
    "        disable_surrounding_nodes(next_nodes, stop_nodes_kd_tree, selected_node, max_walking_dist)\n",
    "        enabled_nodes = [n for n in next_nodes if n.enabled]\n",
    "        orig_node = ox.distance.nearest_nodes(new_graph, selected_node.getLong(), selected_node.getLat()) # Getting the node from the graph itself\n",
    "        old_node = selected_node\n",
    "        selected_node = get_enabled_node_with_highest_edge_probability(selected_node, enabled_nodes)\n",
    "        \n",
    "        if (selected_node == None or selected_node == old_node):\n",
    "            break\n",
    "        \n",
    "        next_nodes.remove(selected_node)\n",
    "        dest_node = ox.distance.nearest_nodes(new_graph, selected_node.getLong(), selected_node.getLat())\n",
    "        \n",
    "        if orig_node is None or dest_node is None:\n",
    "            print(\"Unable to find valid nodes. Please verify the start and end coordinates.\")\n",
    "        elif not nx.has_path(graph, orig_node, dest_node):\n",
    "            print(\"No valid path found between the start and end nodes.\")\n",
    "        else:\n",
    "            \n",
    "            shortest_route = nx.shortest_path(graph, orig_node, dest_node)\n",
    "            distance_travelled = 0\n",
    "            # Get the total distance from point A to point B\n",
    "            for i in range(len(shortest_route)-1):\n",
    "                node_data = graph.nodes[shortest_route[i]]\n",
    "                next_node_data = graph.nodes[shortest_route[i+1]]\n",
    "                \n",
    "                distance_travelled += haversine(node_data['y'], node_data['x'], next_node_data['y'], next_node_data['x'])\n",
    "\n",
    "            # Checks if it does not exceed the max distance\n",
    "            if totalDistance + distance_travelled <= MAX_DISTANCE:\n",
    "                # Add the nodes to the nx graph\n",
    "                new_graph.add_edge(orig_node, dest_node, weight=distance_travelled)\n",
    "                \n",
    "                totalDistance += distance_travelled\n",
    "                used_stops.append(selected_node)\n",
    "                route.append(shortest_route)\n",
    "            else:\n",
    "                break\n",
    "    return route\n",
    "\n",
    "# Disable surrounding nodes\n",
    "def disable_surrounding_nodes(next_nodes, stop_nodes_kd_tree, source_node, max_distance):\n",
    "    source = (source_node.getLat(), source_node.getLong())\n",
    "    \n",
    "    for node in next_nodes:\n",
    "        point = (node.getLat(), node.getLong())\n",
    "        distance = geodesic(source, point).meters\n",
    "        if distance <= max_distance:\n",
    "            node.disable()\n",
    "        \n",
    "def get_enabled_node_with_highest_edge_probability(source_node, enabled_nodes):\n",
    "    highest_edge_prob = 0\n",
    "    highest_edge_prob_node = None\n",
    "\n",
    "    for n in enabled_nodes:\n",
    "        edge_prob = get_edge_probability(source_node, n, len(enabled_nodes))\n",
    "        if edge_prob > highest_edge_prob:\n",
    "            highest_edge_prob = edge_prob\n",
    "            highest_edge_prob_node = n\n",
    "\n",
    "    return highest_edge_prob_node\n",
    "\n",
    "\n",
    "def get_edge_probability(source, destination, normalization_factor):\n",
    "    source_coord = [source.getLat(), source.getLong()]\n",
    "    dest_coord = [destination.getLat(), destination.getLong()]\n",
    "    return exp(-(euclidean(source_coord, dest_coord))) / float(normalization_factor)\n",
    "\n",
    "\n",
    "def radius(stops):\n",
    "    circles = []\n",
    "    for stop in stops:\n",
    "        stop_point = Point(stop[1], stop[0])  # Create a Point object from [lat, lon] coordinates\n",
    "        circle = stop_point.buffer(radius / 111000)  # Buffer the Point to create a circle (assuming 1 degree is approximately 111000 meters)\n",
    "        circles.append(circle)\n",
    "    return circles\n",
    "\n",
    "def enable_stop_nodes(stop_nodes):\n",
    "    for n in stop_nodes:\n",
    "        n.enable()\n",
    "\n",
    "def all_nodes_disabled(stop_nodes):\n",
    "    return get_num_disabled(stop_nodes) == len(stop_nodes)\n",
    "\n",
    "def get_num_disabled(stop_nodes):\n",
    "    return sum(1 for n in stop_nodes if not n.enabled)\n",
    "\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Use geopy's geodesic function to calculate the distance\n",
    "    distance = geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "    return distance\n",
    "\n",
    "# Markers for visualization purposes\n",
    "def add_markers(used_stops):\n",
    "    for stop in used_stops:\n",
    "        #popup_text = f\"Name: {stop.name}<br>Type: {stop.a_type}<br>Coordinates: {stop.getLat()}, {stop.getLong()}\"\n",
    "        folium.Marker(location=[stop.road_lat, stop.road_long]).add_to(m)\n",
    "        \n",
    "        \n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNAPPING ROUTE NETWORK TO GRAPH\n",
    "# NO UUID V1\n",
    "\n",
    "def snap_route_network_to_road(route_network):\n",
    "    location_road_nodes = [node for node, data in graph.nodes(data=True)]\n",
    "    snapped_route_network = []\n",
    "    snapped_route_network_graph = []\n",
    "    \n",
    "    overall_graph = nx.Graph()\n",
    "\n",
    "    route_id = 0\n",
    "    for route in route_network:\n",
    "        snapped_route = snap_route_to_road(location_road_nodes, route)\n",
    "        snapped_route_network_graph.append(snapped_route)\n",
    "        nx.set_edge_attributes(snapped_route, 'route_id', route_id)\n",
    "        route_id += 1\n",
    "        overall_graph = nx.compose(overall_graph, snapped_route)\n",
    "\n",
    "        snapped_edges = list(snapped_route.edges(data='road_path', default=1))\n",
    "        snapped_route = connect_snapped_edges(snapped_edges)\n",
    "        snapped_route_network.append(snapped_route)\n",
    "        \n",
    "    snapped_route_graph = nx.union_all(snapped_route_network_graph)\n",
    "\n",
    "    return snapped_route_network, snapped_route_graph\n",
    "\n",
    "\n",
    "\n",
    "def connect_snapped_edges(snapped_edges):\n",
    "    connected_edge = []\n",
    "\n",
    "    while len(snapped_edges) > 0:\n",
    "        curr_edge = consecutive_connect([], snapped_edges.pop(0))\n",
    "\n",
    "        for e in snapped_edges:\n",
    "            consecutive_edge = consecutive_connect(curr_edge, e)\n",
    "            if consecutive_edge is not None:\n",
    "                curr_edge = consecutive_edge\n",
    "                snapped_edges.remove(e)\n",
    "\n",
    "        new_connected_edge = consecutive_connect(connected_edge, curr_edge)\n",
    "        connected_edge = new_connected_edge if new_connected_edge is not None else connected_edge\n",
    "\n",
    "    return connected_edge\n",
    "\n",
    "def consecutive_connect(e1, e2):\n",
    "    if len(e1) == 0:\n",
    "        return e2\n",
    "    elif len(e2) == 0:\n",
    "        return e1\n",
    "\n",
    "    if e1[-1] == e2[0]:\n",
    "        return e1 + e2\n",
    "    elif e2[-1] == e1[0]:\n",
    "        return e2 + e1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def snap_route_to_road(location_road_nodes, route_stop_nodes):\n",
    "    snapped_route = nx.Graph()\n",
    "    route_stops = [] # We need to store the list of stops as route_stop_nodes is only a list of ox.shortest_path results\n",
    "    \n",
    "    \n",
    "    # Directly add nodes based on node identifiers\n",
    "    for n in route_stop_nodes:\n",
    "        snapped_route.add_node(n[0], **graph.nodes[n[0]])\n",
    "        route_stops.append(n[0])\n",
    "        last_path = n # For the sake of getting the last stop of the last stop connection\n",
    "        \n",
    "    snapped_route.add_node(n[len(last_path)-1], **graph.nodes[n[len(last_path)-1]])\n",
    "    route_stops.append(n[len(last_path)-1])\n",
    "\n",
    "    # Add the shortest path for each consecutive node as an edge in the graph\n",
    "    for i in range(len(route_stops) - 1):\n",
    "        source_node = route_stops[i]\n",
    "        dest_node = route_stops[i + 1]\n",
    "        shortest_road_path = get_shortest_road_path(location_road_nodes, source_node, dest_node)\n",
    "        snapped_route.add_edge(source_node, dest_node, road_path=shortest_road_path)\n",
    "    return snapped_route\n",
    "\n",
    "def get_shortest_road_path(location_road_nodes, source_stop_node, dest_stop_node):\n",
    "    # Find the nearest nodes in the graph to the source and destination GeoPoints\n",
    "    closest_road_node_to_source = ox.distance.nearest_nodes(graph, graph.nodes[source_stop_node]['x'], graph.nodes[source_stop_node]['y'])\n",
    "    closest_road_node_to_dest = ox.distance.nearest_nodes(graph,  graph.nodes[dest_stop_node]['x'], graph.nodes[dest_stop_node]['y'])\n",
    "\n",
    "    if nx.has_path(graph, closest_road_node_to_source, closest_road_node_to_dest):\n",
    "        return nx.shortest_path(graph, closest_road_node_to_source, closest_road_node_to_dest)\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is written in somewhat pseudocode.\n",
    "num_evolutions -> num_generations\n",
    "num_generated_network_mutations_per_evolution -> num_mutations_per_generation\n",
    "mutation_probabilities = list of probabilities for mutation that will be randomly selected from\n",
    "    e.g. [0.1, 0.2, 0.3, 0.4, 0.5] would mean a 0.1 probability for 0 mutations, 0.2 probability for 1 mutation, etc\n",
    "\n",
    "\n",
    "ASSUMPTION: Input is a jeepney route network, represented as a vector of routes, where each route is a vector of stops\n",
    "\"\"\"\n",
    "# population_size - How many networks per generation that we want (set 20 as default)\n",
    "# \n",
    "\n",
    "\n",
    "# This implementation takes only 2 parents from the whole generation and generates the population from them\n",
    "# Instead of the what's in the paper that says the whole population will go through crossovers and mutations\n",
    "# Cite Nayeem et al for GA with elitism and growing population size\n",
    "def perform_genetic_algorithm(network_population, population_size, num_elites, num_generations, mutation_probability, \n",
    "                              num_mutations_probabilities, num_crossovers_probabilities, mutation_threshold_dist, num_failure_removal,\n",
    "                                      weight_random_failure, weight_targeted_failure, weight_connectivity,\n",
    "                              with_elitism=False, with_growing_population=False, num_mutations_per_generation=1):\n",
    "    \n",
    "    \n",
    "    # Do this for the assigned number of generations for the GA\n",
    "    for i in range(num_generations):\n",
    "\n",
    "        new_network_population = []\n",
    "\n",
    "        # Evaluate the fitness of each network in the population\n",
    "        for network in network_population:\n",
    "            list_snapped, road_snapped_network_graph = snap_route_network_to_road(network.routes)\n",
    "            network.fitness_score = compute_fitness_score(road_snapped_network_graph, num_failure_removal,\n",
    "                          weight_random_failure, weight_targeted_failure, weight_connectivity)\n",
    "\n",
    "        sorted_network_population = sorted(network_population, key=lambda x: x.fitness_score, reverse=True)\n",
    "        \n",
    "        # Most naive selection approach: get top two scoring networks as parents\n",
    "        # But should be random with weighted probabilities so that elites are not always parents\n",
    "        \"\"\"\n",
    "        parent1 = sorted_network_population[0]\n",
    "        parent2 = sorted_network_population[1]\n",
    "        \"\"\"\n",
    "\n",
    "        # Roulette Wheel Selection \n",
    "        # Chromosomes with higher fitness have a bigger \"slice of the pie\", but are not \n",
    "        # guaranteed to be selected as parents\n",
    "        # This is to prevent premature convergence and ensure that the best networks are not always selected as parents\n",
    "        max = sum([network.fitness_score for network in sorted_network_population])\n",
    "        selection_p = [network.fitness_score / max for network in sorted_network_population]\n",
    "        parent1_index = np.random.choice(sorted_network_population, 1, p=selection_p)\n",
    "        parent1 = sorted_network_population[parent1_index]\n",
    "        del selection_p[parent1_index]\n",
    "        parent2_index = np.random.choice(np.setdiff1d(sorted_network_population, parent1), 1, p=selection_p)\n",
    "        parent2 = sorted_network_population[parent2_index]\n",
    "\n",
    "        # Take num_elites number of the best networks and automatically add them to the next generation\n",
    "        if (with_elitism):\n",
    "            for i in range(num_elites):\n",
    "                new_network_population.append(sorted_network_population[i])\n",
    "\n",
    "        # Ex: population_size = 20 and num_elites = 2\n",
    "        # If no elitism and no growing population, then we will have 10 iterations to produce 20 in the next generation\n",
    "        # Also, if elitism and growing population, then we will have 10 iterations to produce 22 in the next generation\n",
    "        if (not with_elitism and not with_growing_population or with_elitism and with_growing_population):\n",
    "            num_iterations = population_size / 2\n",
    "\n",
    "        # If with elitism only, maintain the population size and account for the already added elites\n",
    "        elif (with_elitism):  \n",
    "            num_iterations = (population_size - num_elites) / 2\n",
    "\n",
    "        # Generate the population\n",
    "        for i in range(num_iterations):\n",
    "            # Get 2 children from crossovers between the two parents\n",
    "            child1, child2 = crossover_split_index(parent1, parent2)\n",
    "            #child1, child2 = crossover_swap_routes(parent1, parent2, num_crossovers_probabilities)\n",
    "\n",
    "            num_mutations = np.random.choice(len(num_mutations_probabilities), 1, p=num_mutations_probabilities)[0]\n",
    "\n",
    "            for j in range(num_mutations):\n",
    "                # Apply mutations to the children based on mutation probability hyperparameter\n",
    "                if np.random.rand() < mutation_probability:\n",
    "                    child1 = mutate(child1, mutation_threshold_dist)\n",
    "                if np.random.rand() < mutation_probability:\n",
    "                    child2 = mutate(child2, mutation_threshold_dist)\n",
    "            \n",
    "            # Add the children to the new population\n",
    "            new_network_population.append(child1)\n",
    "            new_network_population.append(child2)\n",
    "        # Assign to next generation\n",
    "        network_population = new_network_population\n",
    "\n",
    "    return network_population\n",
    "\n",
    "# This crossover implementation splits both networks at an index and exchanges halves\n",
    "# Assumes that ideally both networks have the same number of routes (same length)\n",
    "def crossover_split_index(network1, network2):\n",
    "    # Split both networks at a random index\n",
    "    if len(network1) < len(network2):\n",
    "        split_index = random.randint(0, len(network2))\n",
    "    else:\n",
    "        split_index = random.randint(0, len(network1))\n",
    "\n",
    "    network1_left = network1[:split_index]\n",
    "    network1_right = network1[split_index:]\n",
    "    network2_left = network2[:split_index]\n",
    "    network2_right = network2[split_index:]\n",
    "\n",
    "    # Swap the right sides of the networks\n",
    "    network1 = network1_left + network2_right\n",
    "    network2 = network2_left + network1_right\n",
    "\n",
    "    return network1, network2\n",
    "\n",
    "# This crossover implementation randomly selects a route from each network and swaps them\n",
    "# More similar to the previous thesis implementation\n",
    "# num_crossovers_probabilities = list of probabilities for crossovers that will be randomly selected from\n",
    "#    e.g. [0.1, 0.2, 0.3, 0.4, 0.5] would mean a 0.1 probability for 0 crossovers, 0.2 probability for 1 crossover, etc\n",
    "def crossover_swap_routes(network1, network2, num_crossovers_probabilities):\n",
    "    num_crossovers = np.random.choice(len(num_crossovers_probabilities), 1, p=num_crossovers_probabilities)[0]\n",
    "\n",
    "    for i in range(num_crossovers):\n",
    "        # Randomly select a route from each network\n",
    "        route1 = random.choice(network1.items())\n",
    "        route2 = random.choice(network2.items())\n",
    "\n",
    "        # Swap the routes\n",
    "        network1[route1[0]] = route2[1]\n",
    "        network2[route2[0]] = route1[1]\n",
    "\n",
    "    return network1, network2\n",
    "\n",
    "\n",
    "# Modify the stop connections of a random route in the network\n",
    "# Randomly select a route and randomly select a stop in that route\n",
    "# Then randomly select another stop that is a not too far from the selected stop based on threshold\n",
    "# Swap connections with that stop\n",
    "def mutate(network, threshold_dist):\n",
    "    # Randomly select a route\n",
    "    random_route = np.random.choice(network.items())\n",
    "\n",
    "    # Randomly select a stop in the route\n",
    "    random_stop_index = np.random.choice(len(random_route))\n",
    "    random_stop = random_route[random_stop_index]\n",
    "\n",
    "    # Will try searching for a random stop 50 times (arbitrary)\n",
    "    for i in range(50):\n",
    "        other_random_route = np.random.choice(network.items())\n",
    "        other_random_stop_index = np.random.choice(len(other_random_route))\n",
    "        other_random_stop = other_random_route[other_random_stop_index]\n",
    "\n",
    "        # Uses the haversine formula but this might screw things up,\n",
    "        # change distance formula as necessary\n",
    "        if haversine(random_stop, other_random_stop) < threshold_dist:\n",
    "            # Swap connections\n",
    "            random_route[random_stop_index] = other_random_stop\n",
    "            other_random_route[other_random_stop_index] = random_stop\n",
    "            break\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness function\n",
    "\n",
    "def select_highest_scoring_mutation(candidate_road_snapped_networks, num_failure_removal,\n",
    "                                    weight_random_failure, weight_targeted_failure, weight_radius_of_gyration):\n",
    "    max_fitness_score = -np.inf\n",
    "    max_candidate_route_snapped_network = None\n",
    "\n",
    "    for n in candidate_road_snapped_networks:\n",
    "        fitness_score = compute_fitness_score(n, num_failure_removal,\n",
    "                                              weight_random_failure, weight_targeted_failure, weight_radius_of_gyration)\n",
    "        if fitness_score > max_fitness_score:\n",
    "            max_fitness_score = fitness_score\n",
    "            max_candidate_route_snapped_network = n\n",
    "\n",
    "    return max_candidate_route_snapped_network\n",
    "\n",
    "def compute_fitness_score(road_snapped_network_graph, num_failure_removal,\n",
    "                          weight_random_failure, weight_targeted_failure, weight_radius_of_gyration):\n",
    "\n",
    "    random_failure_robustness = compute_random_failure_robustness(road_snapped_network_graph, num_failure_removal)\n",
    "    weighted_random_failure_robustness = weight_random_failure * random_failure_robustness\n",
    "\n",
    "    targeted_failure_robustness = compute_targeted_failure_robustness(road_snapped_network_graph, num_failure_removal)\n",
    "    weighted_targeted_failure_robustness = weight_targeted_failure * targeted_failure_robustness\n",
    "\n",
    "    radius_of_gyration = compute_radius_of_gyration(road_snapped_network_graph, 100, weight_radius_of_gyration)\n",
    "    weighted_radius_of_gyration = weight_radius_of_gyration * radius_of_gyration\n",
    "\n",
    "    # Will use this return for now to utilize target and random failure nodes \n",
    "    return weighted_radius_of_gyration - weighted_random_failure_robustness - weighted_targeted_failure_robustness\n",
    "    # return weighted_radius_of_gyration\n",
    "\n",
    "\n",
    "### WRITTEN IN PSEUDOCODE\n",
    "def compute_connectivity(network):\n",
    "    # External connectivity - measure how connected is the jeepney route network with other modes of transpo\n",
    "    \n",
    "    # Get the ratio of transportation stops to total stops in the network\n",
    "    transpo_stops = [stop for stop in network.reshape(-1) if stop.isTranspo]\n",
    "    total_stops = len(network.reshape(-1))\n",
    "    transpo_stop_ratio = len(transpo_stops) / total_stops\n",
    "\n",
    "    # Get the average degree of all transportation stops in the network\n",
    "    avg_transpo_degree = sum(stop.degree for stop in transpo_stops) / len(transpo_stops)\n",
    "\n",
    "    # Find a way to normalize the two values and combine them \n",
    "\n",
    "\n",
    "    # Internal connectivity - measure how connected is each jeepney route to other jeepney routes\n",
    "\n",
    "    # Count number of route intersections in the network\n",
    "    intersections = set()\n",
    "    for i, route in network:\n",
    "        for j, other_route in network:\n",
    "            if i != j and route.intersects(other_route):\n",
    "                intersection_key = tuple(sorted([i, j]))\n",
    "                if intersection_key not in intersections:\n",
    "                    intersections.add(intersection_key)\n",
    "\n",
    "    num_intersections = len(intersections)\n",
    "    \n",
    "    # Change these weights based on what the expected values for \n",
    "    # the transpo_stop_ratio, avg_transpo_degree, and num_intersections will be\n",
    "    external_weight = 0.5\n",
    "    internal_weight = 0.5\n",
    "\n",
    "    # Formula subject to change\n",
    "    return external_weight * (transpo_stop_ratio * avg_transpo_degree) + internal_weight * num_intersections\n",
    "\n",
    "\n",
    "def compute_random_failure_robustness(road_snapped_network_graph, num_removals):\n",
    "    print(road_snapped_network_graph)\n",
    "    print(\"That was the graph\")\n",
    "    for i in range(num_removals):\n",
    "        print(\"Random node \", i)\n",
    "        selected_node = random.choice(list(road_snapped_network_graph.nodes()))\n",
    "        print(selected_node)\n",
    "        road_snapped_network_graph.remove_node(selected_node)\n",
    "\n",
    "    diameter, avg_path_length = compute_network_statistics(road_snapped_network_graph)\n",
    "    return compute_failure_robustness(road_snapped_network_graph, diameter)\n",
    "\n",
    "def compute_targeted_failure_robustness(road_snapped_network_graph, num_removals):\n",
    "    for i in range(num_removals):\n",
    "        node_degrees = road_snapped_network_graph.degree()\n",
    "        # Iterate over the DegreeView object to find the maximum degree\n",
    "        max_degree = max(degree for _, degree in node_degrees)\n",
    "        print(node_degrees)\n",
    "        max_degree_node = get_node_with_degree(node_degrees, max_degree)\n",
    "        road_snapped_network_graph.remove_node(max_degree_node)\n",
    "\n",
    "    diameter, avg_path_length = compute_network_statistics(road_snapped_network_graph)\n",
    "    return compute_failure_robustness(road_snapped_network_graph, diameter)\n",
    "\n",
    "def compute_failure_robustness(road_snapped_network_graph, max_path_length):\n",
    "    return float(max_path_length) / float(len(road_snapped_network_graph) - 1)\n",
    "\n",
    "def compute_radius_of_gyration(road_snapped_network_graph, num_random_values, weight):\n",
    "    return _get_efficiency_sum(road_snapped_network_graph, num_random_values, weight)\n",
    "\n",
    "def _get_efficiency_sum(graph, no_of_random_values, weight):\n",
    "    efficiency_sum = 0.0\n",
    "    weighted_list = _get_yweighted_list(graph, weight)\n",
    "    efficiency_sum_list = random.sample(weighted_list.keys(), no_of_random_values)\n",
    "\n",
    "    for k_x, k_y in efficiency_sum_list:\n",
    "         temp = weighted_list[(str(k_x), str(k_y))]\n",
    "         efficiency_sum = float(temp) + float(efficiency_sum)\n",
    "\n",
    "    return efficiency_sum\n",
    "\n",
    "def _get_yweighted_list(graph, weight):\n",
    "    dp = _get_distance_individual(graph)\n",
    "    dw = _get_total_weighted_distance(graph, weight)\n",
    "    Y = {}\n",
    "\n",
    "    for k_x, v_x in graph.nodes(data=True):\n",
    "        for k_y, v_y in graph.nodes(data=True):\n",
    "            if nx.has_path(graph, k_x, k_y):\n",
    "                Y[(str(k_x), str(k_y))] = float(dp[(str(k_x), str(k_y))])/float(dw)\n",
    "            else:\n",
    "                Y[(str(k_x), str(k_y))] = 0.0\n",
    "\n",
    "    return Y\n",
    "\n",
    "def _get_distance_individual(graph):\n",
    "    T = {}\n",
    "\n",
    "    for k_x, v_x in graph.nodes(data=True):\n",
    "        for k_y, v_y in graph.nodes(data=True):\n",
    "            if nx.has_path(graph, k_x, k_y):\n",
    "                shortest_path_nodes = nx.shortest_path(graph, k_x, k_y)\n",
    "                accumulated_distance = 0.0\n",
    "                if len(shortest_path_nodes) > 1:\n",
    "                    for i in range(0, len(shortest_path_nodes) - 1):\n",
    "                        edge = graph.get_edge_data(shortest_path_nodes[i], shortest_path_nodes[i + 1]).get('dist', 0)\n",
    "                        print(edge)\n",
    "                        accumulated_distance += float(edge)\n",
    "                    T[(k_x, k_y)] = accumulated_distance\n",
    "                else:\n",
    "                    T[(k_x, k_y)] = 0\n",
    "            else:\n",
    "                T[(k_x, k_y)] = 0\n",
    "\n",
    "    return T\n",
    "\n",
    "# new gettwd does not use weighted adjacency matrix\n",
    "def _get_total_weighted_distance(graph, weight):\n",
    "    # A = _create_weighted_adjacency_matrix(graph)\n",
    "    dp = _get_distance_individual(graph)\n",
    "    w = weight\n",
    "    total_weighted_distance = 0.0\n",
    "    T = {}\n",
    "    for k_x, v_x in graph.nodes(data=True):\n",
    "        for k_y, v_y in graph.nodes(data=True):\n",
    "            if nx.has_path(graph, k_x, k_y):\n",
    "                shortest_path_nodes = nx.shortest_path(graph, k_x, k_y)\n",
    "                g = get_nodes_shortest_path(shortest_path_nodes, graph)\n",
    "                T = _get_no_of_transfers(g)\n",
    "                a = 1.0\n",
    "            elif not nx.has_path(graph, k_x, k_y):\n",
    "                a = 10.0\n",
    "\n",
    "            b = float(dp[(str(k_x), str(k_y))])\n",
    "            weighted_distance = a * b + (w * T)\n",
    "            total_weighted_distance = float(total_weighted_distance) + float(weighted_distance)\n",
    "\n",
    "    return total_weighted_distance\n",
    "\n",
    "def compute_network_statistics(road_snapped_network_graph):\n",
    "    path_lengths = get_path_lengths(road_snapped_network_graph)\n",
    "    print(path_lengths)\n",
    "    avg_path_length = np.mean(path_lengths)\n",
    "    max_path_length = max(path_lengths)\n",
    "\n",
    "    #network_size = len(path_lengths)\n",
    "    #gcc = sorted(nx.connected_component_subgraphs(road_snapped_network_graph), key=len, reverse=True)\n",
    "    #giant_component_fraction = float(float(gcc[0].order()) / float(network_size))\n",
    "    #return max_path_length, avg_path_length, giant_component_fraction\n",
    "    return max_path_length, avg_path_length\n",
    "\n",
    "def get_path_lengths(snapped_road_network_graph):\n",
    "    return [sum(nx.single_source_shortest_path_length(snapped_road_network_graph, n).values())\n",
    "            for n in snapped_road_network_graph]\n",
    "    \n",
    "def get_node_with_degree(node_degrees, degree):\n",
    "    # Iterate over the DegreeView object to find the node with the specified degree\n",
    "    for node, _ in node_degrees:\n",
    "        if _ == degree:\n",
    "            return node\n",
    "    return None  # Return None if no node with the specified degree is found\n",
    "\n",
    "def get_nodes_shortest_path(shortest_path_nodes, graph):\n",
    "    new_graph = nx.Graph()\n",
    "\n",
    "    for k_y, v_y in graph.nodes(data=True):\n",
    "        for elem in shortest_path_nodes:\n",
    "            if elem == k_y:\n",
    "                new_graph.add_node(k_y, lat=v_y.get('lat'), lon=v_y.get('lon'), route_id = v_y.get('route_id'))\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def _get_no_of_transfers(graph):\n",
    "    temp = []\n",
    "    p = graph.copy()\n",
    "    no_of_tranfer = 0\n",
    "\n",
    "    for k_y, v_y in p.nodes(data=True):\n",
    "        if (len(p.nodes()) > 1):\n",
    "            if str(v_y.get(\"route_id\")) not in temp:\n",
    "                temp.append(str(v_y.get(\"route_id\")))\n",
    "            no_of_transfer = len(temp)-1\n",
    "        else:\n",
    "            no_of_tranfer = 0\n",
    "\n",
    "    return no_of_tranfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING INITIAL NETWORK\n",
    "\n",
    "manila_amenities_network = create_network(manila_amenities_polygon_gdf, manila_amenities_point_gdf)\n",
    "\n",
    "# Make a before map\n",
    "before_map = plot_network_on_map(manila_amenities_network, initial_location=[0, 0], zoom_start=100)\n",
    "before_map.save('before_map.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LEVEL 1 - CONNECTING POLYGONS OF SAME AMENITY\n",
    "\n",
    "combined_graph = combine_amenities_by_polygon(manila_amenities_network, max_distance=100, max_perimeter=10000)\n",
    "after_map = plot_network_on_map(combined_graph, initial_location=[0, 0], zoom_start=100)\n",
    "after_map.save('after_map.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING POPULATION DENSITY OF RESIDENTIAL AREAS\n",
    "\n",
    "pop_graph = check_residential_population_density(graph=combined_graph, threshold=100)\n",
    "pop_map = plot_population_zones_map(pop_graph, initial_location=[0, 0], zoom_start=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL 2 - CREATING NETWORKS OF AMENITIES\n",
    "\n",
    "graph_networks_of_polygons = create_zone_network(graph=combined_graph, max_distance=100)\n",
    "networks_map = plot_connected_zones_network_on_map(graph_networks_of_polygons, initial_location=[0, 0], zoom_start=100)\n",
    "networks_map.save('networks_map.html') # Save the map to an HTML file\n",
    "\n",
    "feature_collection = graph_to_geojson(manila_amenities_network, 'output.geojson')\n",
    "with open('output.geojson', 'w', encoding='utf-8') as f:\n",
    "    f.write(geojson.dumps(feature_collection, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEVEL 3 - CREATING STOPS TO BE PLACED ON ZONES\n",
    "graph_of_stops = nx.Graph()\n",
    "list_of_stops = []\n",
    "place_stops_on_roads(graph_networks_of_polygons)\n",
    "\n",
    "# Visualize the stops\n",
    "stops_map = plot_stops_on_map(networks_map, list_of_stops, initial_location=[0, 0], zoom_start=100)\n",
    "stops_map.save('stops_map.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export stops to pickle\n",
    "\n",
    "# Specify the file path where you want to save the pickle file\n",
    "file_path = 'stop_objects.pkl'\n",
    "\n",
    "# Open the file in binary write mode\n",
    "with open(file_path, 'wb') as f:\n",
    "    # Dump the list of stop objects into the pickle file\n",
    "    pickle.dump(stops, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Export networks or graphs to pickle\n",
    "def export_networks(networks, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(networks, f)\n",
    "\n",
    "def import_networks(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        routes = pickle.load(f)\n",
    "    return routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LEVEL 4 - CONNECTING STOPS INTO A NETWORK\n",
    "WALKING_DISTANCES = [300,550,800]\n",
    "MAX_DISTANCE = 15\n",
    "\n",
    "# Configuration\n",
    "set_walk_distance = WALKING_DISTANCES[0]\n",
    "num_of_networks = 10\n",
    "        \n",
    "# Generate route network\n",
    "list_of_networks = []\n",
    "list_of_graphs = []\n",
    "\n",
    "for _ in range(num_of_networks):\n",
    "    route_count = 0\n",
    "    used_stops = []\n",
    "    route_network, new_graph = generate_route_network(list_of_stops, set_walk_distance) # Default max walking distance is 300m\n",
    "    \n",
    "    # or a network object\n",
    "    list_of_networks.append(network(route_network, used_stops))\n",
    "    list_of_graphs.append(new_graph)\n",
    "\n",
    "\n",
    "#Export networks and graphs using pickl\n",
    "export_networks(list_of_networks, \"networks.pkl\")\n",
    "export_networks(list_of_graphs, \"graphs.pkl\")\n",
    "\n",
    "\n",
    "i = 1\n",
    "# Creating Maps for visualization\n",
    "for route_network in list_of_networks:\n",
    "    map_center = (14.599512, 120.984222)\n",
    "    m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "    # Plotting in the Map\n",
    "    add_markers(route_network.stops)\n",
    "        \n",
    "    for route in route_network.routes:\n",
    "        for connection in route:\n",
    "            ox.plot_route_folium(CITY_GRAPH, connection, route_map=m, tiles='openstreetmap', route_color=\"green\")\n",
    "\n",
    "    m.save(f\"Map2-{i}.html\")\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING GA ---------------------------------------------------------]\n",
    "# There should a pickle file already of the latest networks\n",
    "\n",
    "\n",
    "# Import networks into list_of_networks\n",
    "# import pickl\n",
    "list_of_networks = import_networks(\"networks.pkl\")\n",
    "#list_of_graphs = import_networks(\"graphs.pkl\")\n",
    "\n",
    "population_size = 20 # Default\n",
    "num_elites = 2\n",
    "num_generations = 10\n",
    "mutation_probability = 0.1\n",
    "num_mutations_probabilities = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "num_crossovers_probabilities = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "mutation_threshold_dist = 300\n",
    "with_elitism = False\n",
    "with_growing_population = False\n",
    "num_mutations_per_generation = 2\n",
    "\n",
    "#Weights and Fitness Function configuration\n",
    "num_failure_removal = 4\n",
    "weight_random_failure = 0.2\n",
    "weight_targeted_failure = 0.2\n",
    "weight_connectivity = 0.6\n",
    "\n",
    "# NOTE: USING list_of_graphs for the GA\n",
    "population = perform_genetic_algorithm(list_of_networks, population_size, num_elites, num_generations, mutation_probability, \n",
    "                              num_mutations_probabilities, num_crossovers_probabilities, mutation_threshold_dist, num_failure_removal,\n",
    "                                      weight_random_failure, weight_targeted_failure, weight_connectivity,\n",
    "                              with_elitism, with_growing_population, num_mutations_per_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial index for points\n",
    "idx = index.Index()\n",
    "for j, point in merged_amenities_points_gdf.iterrows():\n",
    "    idx.insert(j, point['geometry'].bounds)\n",
    "\n",
    "# Iterate over polygons\n",
    "for i, polygon in merged_amenities_polygons_gdf.iterrows():\n",
    "    points_within_polygon = []\n",
    "    \n",
    "    # Iterate over points within the bounding box of the polygon\n",
    "    for j in idx.intersection(polygon['geometry'].bounds):\n",
    "        point = merged_amenities_points_gdf.loc[j]\n",
    "        if polygon['geometry'].intersects(point['geometry']):\n",
    "            points_within_polygon.append(j)\n",
    "    \n",
    "    merged_amenities_polygons_gdf.at[i, 'amenity_points'] = points_within_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manila_amenities_polygon_gdf['amenity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_amenities_polygons_gdf['amenity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Initial network\n",
    "merged_amenities_network = create_network(merged_amenities_polygons_gdf, merged_amenities_points_gdf)\n",
    "\n",
    "# Make a before map\n",
    "merge_map = plot_network_on_map(merged_amenities_network, initial_location=[0, 0], zoom_start=100)\n",
    "merge_map.save('merge_map.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST TO VISUALIZE RIVERS AND STREAMS\n",
    "map_center = (14.599512, 120.984222)  # TEMPORARY WILL ZOOM TO MANILA\n",
    "m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "# Iterate over the nodes in the network\n",
    "folium.GeoJson(filtered_rivers, style_function=lambda x: {'color': 'blue'}).add_to(m)\n",
    "\n",
    "m.save('check.html')  # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting polygons of same amenity\n",
    "connected_lines = []\n",
    "combined_graph = combine_amenities_by_polygon(manila_amenities_network, max_distance=100, max_perimeter=10000)\n",
    "after_map = plot_network_on_map(combined_graph, initial_location=[0, 0], zoom_start=100)\n",
    "\n",
    "\n",
    "# The lines to show the networks\n",
    "for line in connected_lines:\n",
    "    line_coords = [[coord[1], coord[0]] for coord in line.coords]\n",
    "    folium.PolyLine(locations=line_coords, color='black').add_to(after_map)\n",
    "after_map.save('after_map.html') # Save the map to an HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC (VISUALIZATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VISUALIZATION ONLY\n",
    "all_roads_map = plot_all_roads()\n",
    "all_roads_map.save('all_roads.html')\n",
    "\n",
    "filtered_road_map = plot_all_filtered_roads()\n",
    "filtered_road_map.save('filtered_road_map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_amenities_points_gdf['amenity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "m = folium.Map(location=map_center, zoom_start=10, tiles='openstreetmap')\n",
    "\n",
    "for j, point in merged_amenities_points_gdf.iterrows():\n",
    "    if point['amenity'] == 'grocery':\n",
    "        folium.Marker(location=[point['y'], point['x']], popup=f\"{point['name']}\").add_to(m)\n",
    "        \n",
    "m.save('test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amenity_test(amenities_network, amenity, initial_location=[0, 0], zoom_start=10):\n",
    "    # Create a map centered at the initial location\n",
    "    map_center = (14.599512, 120.984222) # TEMPORARY WILL ZOOM TO MANILA\n",
    "    m = folium.Map(location=map_center, zoom_start=zoom_start, tiles='openstreetmap')\n",
    "    \n",
    "    #Colours for Visualization\n",
    "    amenity_colors = {\n",
    "        'education': 'green',\n",
    "        'finance': 'blue',\n",
    "        'government offices': 'red',\n",
    "        'grocery': 'orange',\n",
    "        'health': 'magenta',\n",
    "        'malls': 'yellow',\n",
    "        'residential areas': 'brown',\n",
    "        'security': 'gray',\n",
    "        'transportation': 'lightblue',\n",
    "        'others': 'black'\n",
    "    }\n",
    "\n",
    "    # Iterate over the nodes in the network\n",
    "    for node, data in amenities_network.nodes(data=True):\n",
    "        if data['amenity'] == amenity:\n",
    "            # Check if the node has a geometry attribute\n",
    "            if 'geometry' in data:\n",
    "                # Get the geometry of the node\n",
    "                geometry = data['geometry']\n",
    "\n",
    "                # Check the geometry type and plot accordingly\n",
    "                if geometry.geom_type == 'Point':\n",
    "                    # Plot a marker for points    \n",
    "                    #folium.Marker(location=[geometry.y, geometry.x], popup=f\"{data['name']}\").add_to(m)\n",
    "                    continue\n",
    "                elif geometry.geom_type in ['Polygon', 'MultiPolygon']:\n",
    "                    # Plot polygons or multipolygons\n",
    "                    color = amenity_colors[data.get('amenity')]\n",
    "                    if geometry.geom_type == 'Polygon':\n",
    "                        polygons = [geometry]\n",
    "                    else:\n",
    "                        polygons = geometry.geoms\n",
    "\n",
    "                    for polygon in polygons:\n",
    "                        coordinates = []\n",
    "                        for point in polygon.exterior.coords:\n",
    "                            coordinates.append([point[1], point[0]])\n",
    "                        folium.Polygon(locations=coordinates, fill=True, color=color, fill_opacity=0.4).add_to(m)\n",
    "\n",
    "    # Return the map\n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
